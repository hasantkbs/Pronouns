# evaluate_model.py

import os
import pandas as pd
import torch
import librosa
from transformers import WhisperForConditionalGeneration, WhisperProcessor
from datasets import Dataset, Audio
import evaluate
from pathlib import Path
import config
from tqdm import tqdm

class ModelEvaluator:
    def __init__(self, user_id, model_path=None):
        self.user_id = user_id
        self.personalized_model_dir = Path("data/models/personalized_models") / self.user_id
        self.data_path = Path(config.BASE_PATH) / self.user_id / "metadata_words.csv"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Whisper modeline gÃ¶re base_model_name'i sabitliyoruz.
        self.base_model_name = "openai/whisper-large-v2"

        print(f"âœ… {self.user_id} iÃ§in kiÅŸiselleÅŸtirilmiÅŸ model yÃ¼klenecek: {self.personalized_model_dir}")
        
        # Whisper Processor'Ä± yÃ¼klÃ¼yoruz
        self.processor = WhisperProcessor.from_pretrained(self.base_model_name, language="tr", task="transcribe")
        
        # Temel Whisper modelini ve Ã¼zerine eÄŸitilmiÅŸ PEFT adaptÃ¶rÃ¼nÃ¼ yÃ¼klÃ¼yoruz
        from peft import PeftModel
        base_model = WhisperForConditionalGeneration.from_pretrained(self.base_model_name)
        self.model = PeftModel.from_pretrained(base_model, str(self.personalized_model_dir))
            
        self.model.to(self.device)
        self.wer_metric = evaluate.load("wer")
        self.cer_metric = evaluate.load("cer")

    def prepare_dataset(self, max_samples=None):
        """
        DeÄŸerlendirme veri setini hazÄ±rlar.
        
        Args:
            max_samples: Maksimum Ã¶rnek sayÄ±sÄ± (None ise tÃ¼mÃ¼)
        
        Returns:
            Dataset veya None (hata durumunda)
        """
        # Ã–nce eval.csv'yi kontrol et
        eval_csv = Path(config.BASE_PATH) / self.user_id / "eval.csv"
        if eval_csv.exists():
            print(f"   âœ… eval.csv bulundu, kullanÄ±lÄ±yor.")
            df = pd.read_csv(eval_csv, encoding='utf-8')
        elif self.data_path.exists():
            print(f"   âš ï¸  eval.csv bulunamadÄ±, metadata_words.csv kullanÄ±lÄ±yor.")
            df = pd.read_csv(self.data_path, encoding='utf-8')
            df = df[['file_path', 'transcription']].copy()
            df.rename(columns={'transcription': 'transcript'}, inplace=True)
        else:
            print(f"âŒ Hata: Ne eval.csv ne de {self.data_path} bulunamadÄ±.")
            return None

        # Maksimum Ã¶rnek sayÄ±sÄ± sÄ±nÄ±rÄ±
        if max_samples and len(df) > max_samples:
            print(f"   âš ï¸  {len(df)} Ã¶rnek var, {max_samples} ile sÄ±nÄ±rlandÄ±rÄ±lÄ±yor.")
            df = df.head(max_samples)

        # Dosya yollarÄ±nÄ± dÃ¼zelt
        words_dir = Path(config.BASE_PATH) / self.user_id / "words"
        df["file_path"] = df["file_path"].apply(
            lambda x: str(words_dir / os.path.basename(str(x)))
        )
        
        # Var olmayan dosyalarÄ± filtrele
        original_size = len(df)
        df = df[df["file_path"].apply(os.path.exists)]
        if len(df) < original_size:
            print(f"   âš ï¸  {original_size - len(df)} adet bulunamayan ses dosyasÄ± atlandÄ±.")

        if len(df) == 0:
            print(f"âŒ Hata: HiÃ§ geÃ§erli ses dosyasÄ± bulunamadÄ±!")
            return None

        # Transcript sÃ¼tununu kontrol et
        transcript_col = 'transcript' if 'transcript' in df.columns else 'transcription'
        df = df[df[transcript_col].notna() & (df[transcript_col].str.strip() != '')]

        dataset = Dataset.from_pandas(df).cast_column(
            "file_path", 
            Audio(sampling_rate=config.ORNEKLEME_ORANI, decode=False)
        )
        
        print(f"   ğŸ“Š DeÄŸerlendirme seti: {len(dataset)} Ã¶rnek")
        return dataset

    def evaluate_model(self, dataset, max_samples=None):
        """
        Modeli deÄŸerlendirir ve WER/CER metriklerini hesaplar.
        
        Args:
            dataset: DeÄŸerlendirme veri seti
            max_samples: Maksimum deÄŸerlendirilecek Ã¶rnek sayÄ±sÄ±
        """
        if dataset is None:
            print("âŒ DeÄŸerlendirme veri seti yok!")
            return
        
        from torch.utils.data import DataLoader

        # Whisper iÃ§in Ã¶zel collate fonksiyonu
        def collate_fn(batch):
            input_features = []
            labels = []
            
            for item in batch:
                try:
                    # Audio yÃ¼kleme, processor input_features'Ä± oluÅŸturacak
                    audio_input = item['file_path']['array'] # already loaded by datasets
                    
                    # Transcription tokenization
                    label = self.processor.tokenizer(item['transcript']).input_ids
                    
                    input_features.append({"input_features": audio_input})
                    labels.append({"input_ids": label})

                except Exception as e:
                    print(f"âš ï¸  Ses veya transkript iÅŸlenirken hata: {e}")
                    continue
            
            if not input_features:
                return None
            
            # Input features'Ä± batch halinde hazÄ±rla
            batch_input_features = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
            
            # Labels'Ä± batch halinde hazÄ±rla ve padding tokenlarÄ±nÄ± -100 ile deÄŸiÅŸtir
            batch_labels = self.processor.tokenizer.pad(labels, return_tensors="pt")
            batch_labels["input_ids"] = batch_labels["input_ids"].masked_fill(
                batch_labels.attention_mask.ne(1), -100
            )

            return {
                "input_features": batch_input_features.input_features,
                "labels": batch_labels.input_ids,
                "attention_mask": batch_input_features.attention_mask
            }

        dataloader = DataLoader(
            dataset, 
            batch_size=config.FINETUNE_BATCH_SIZE, # config'den al
            collate_fn=collate_fn,
            num_workers=config.DATALOADER_NUM_WORKERS,
            pin_memory=config.DATALOADER_PIN_MEMORY,
            prefetch_factor=config.DATALOADER_PREFETCH_FACTOR if config.DATALOADER_NUM_WORKERS > 0 else None,
            persistent_workers=True if config.DATALOADER_NUM_WORKERS > 0 else False
        )

        predictions = []
        references = []

        print(f"\nğŸš€ {self.user_id} modeli deÄŸerlendiriliyor...")
        print(f"   Toplam kayÄ±t: {len(dataset)}")
        
        self.model.eval()
        processed_count = 0
        
        try:
            with torch.no_grad():
                for batch in tqdm(dataloader, desc="DeÄŸerlendirme"):
                    if batch is None:
                        continue
                    
                    input_features = batch["input_features"].to(self.device)
                    # Whisper modeli iÃ§in generate metodunu kullanÄ±yoruz
                    generated_ids = self.model.generate(
                        input_features=input_features,
                        # attention_mask=batch.get("attention_mask", None).to(self.device), # Whisper'da generate iÃ§in attention_mask gerekli deÄŸil
                        language="tr", # TÃ¼rkÃ§e dilini belirt
                        task="transcribe", # Transkripsiyon gÃ¶revi
                        return_timestamps=False # Zaman damgalarÄ±nÄ± dÃ¶ndÃ¼rme
                    )
                    
                    # Tahminleri Ã§Ã¶zÃ¼mlÃ¼yoruz
                    transcription = self.processor.batch_decode(
                        generated_ids, 
                        skip_special_tokens=True
                    )

                    # Referans metinleri Ã§Ã¶zÃ¼mlÃ¼yoruz (padding'i kaldÄ±rarak)
                    labels = batch["labels"].cpu().numpy()
                    labels[labels == -100] = self.processor.tokenizer.pad_token_id
                    reference_texts = self.processor.batch_decode(
                        labels, 
                        skip_special_tokens=True
                    )

                    predictions.extend(transcription)
                    references.extend(reference_texts)
                    processed_count += len(transcription)
                    
                    if max_samples and processed_count >= max_samples:
                        break

            if len(predictions) == 0:
                print("âŒ HiÃ§ tahmin yapÄ±lamadÄ±!")
                return

            # Metrikleri hesapla
            wer = self.wer_metric.compute(
                predictions=predictions, 
                references=references
            )
            cer = self.cer_metric.compute(
                predictions=predictions, 
                references=references
            )

            print("\n" + "="*50)
            print(f"âœ… DeÄŸerlendirme TamamlandÄ±!")
            print(f"   Ä°ÅŸlenen Ã¶rnek: {len(predictions)}")
            print(f"   Word Error Rate (WER): {wer:.4f} ({wer*100:.2f}%)")
            print(f"   Character Error Rate (CER): {cer:.4f} ({cer*100:.2f}%)")
            print("="*50)
            
            # Ã–rnek tahminler gÃ¶ster
            if len(predictions) > 0:
                print("\nğŸ“ Ã–rnek Tahminler:")
                for i in range(min(5, len(predictions))):
                    print(f"   {i+1}. GerÃ§ek: '{references[i]}'")
                    print(f"      Tahmin: '{predictions[i]}'")
                    print()

            self.provide_suggestions(wer, cer)
            
        except Exception as e:
            print(f"\nâŒ DeÄŸerlendirme sÄ±rasÄ±nda hata: {e}")
            import traceback
            traceback.print_exc()

    def provide_suggestions(self, wer, cer):
        print("\nğŸ’¡ GeliÅŸtirme Ã–nerileri:")
        if wer > 0.3 or cer > 0.15: # Arbitrary thresholds for "poor" performance
            print("   - Daha fazla ve Ã§eÅŸitli veri toplayÄ±n.")
            print("   - `train_adapter.py` iÃ§indeki eÄŸitim parametrelerini (epoch, Ã¶ÄŸrenme oranÄ±) ayarlamayÄ± deneyin.")
        elif wer > 0.15 or cer > 0.05:
            print("   - Model performansÄ± iyi gÃ¶rÃ¼nÃ¼yor. Daha fazla veri ile daha da iyileÅŸtirilebilir.")
        else:
            print("   - Model performansÄ± oldukÃ§a baÅŸarÄ±lÄ±!")

import argparse

def main():
    parser = argparse.ArgumentParser(
        description="KiÅŸiselleÅŸtirilmiÅŸ ASR modelini deÄŸerlendirir."
    )
    parser.add_argument(
        "user_id", 
        type=str, 
        help="DeÄŸerlendirilecek kullanÄ±cÄ±nÄ±n kimliÄŸi (Ã¶rn: Furkan)"
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="Maksimum deÄŸerlendirilecek Ã¶rnek sayÄ±sÄ± (varsayÄ±lan: tÃ¼mÃ¼)"
    )
    
    args = parser.parse_args()

    print("="*50)
    print(f"Model DeÄŸerlendirme: {args.user_id}")
    print("="*50)
    
    evaluator = ModelEvaluator(user_id=args.user_id)
    dataset = evaluator.prepare_dataset(max_samples=args.max_samples)
    if dataset:
        evaluator.evaluate_model(dataset, max_samples=args.max_samples)
    else:
        print("âŒ DeÄŸerlendirme yapÄ±lamadÄ±!")

if __name__ == "__main__":
    main()