#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
EÄŸitim verisi hazÄ±rlama scripti.
KullanÄ±cÄ±nÄ±n metadata_words.csv dosyasÄ±ndan train.csv ve eval.csv dosyalarÄ±nÄ± oluÅŸturur.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import os
import argparse
from pathlib import Path
import config

def prepare_training_data(user_id, test_size=0.2, random_state=42):
    """
    KullanÄ±cÄ± verilerini eÄŸitim ve deÄŸerlendirme setlerine ayÄ±rÄ±r.
    
    Args:
        user_id: KullanÄ±cÄ± kimliÄŸi (Ã¶rn: "Furkan")
        test_size: DeÄŸerlendirme seti oranÄ± (varsayÄ±lan: 0.2 = %20)
        random_state: Rastgelelik tohumu (tekrarlanabilirlik iÃ§in)
    
    Returns:
        tuple: (train_df, eval_df) veya (None, None) hata durumunda
    """
    base_path = Path(config.BASE_PATH)
    user_path = base_path / user_id
    
    # Metadata dosyasÄ±nÄ± kontrol et
    metadata_path = user_path / "metadata_words.csv"
    if not metadata_path.exists():
        print(f"âŒ Hata: {metadata_path} bulunamadÄ±!")
        return None, None
    
    print(f"ğŸ“Š Veri hazÄ±rlama baÅŸlatÄ±lÄ±yor: {user_id}")
    print(f"   Metadata dosyasÄ±: {metadata_path}")
    
    try:
        # Metadata'yÄ± yÃ¼kle
        df = pd.read_csv(metadata_path, encoding='utf-8')
        print(f"   Toplam kayÄ±t sayÄ±sÄ±: {len(df)}")
        
        # Gerekli sÃ¼tunlarÄ± kontrol et
        required_columns = ['file_path', 'transcription']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            print(f"âŒ Hata: Eksik sÃ¼tun(lar): {missing_columns}")
            return None, None
        
        # Dosya yollarÄ±nÄ± dÃ¼zelt (words klasÃ¶rÃ¼ne gÃ¶re)
        words_dir = user_path / "words"
        def fix_file_path(path):
            """Dosya yolunu dÃ¼zeltir."""
            if pd.isna(path):
                return None
            # EÄŸer zaten tam yol deÄŸilse, words klasÃ¶rÃ¼ne gÃ¶re dÃ¼zelt
            filename = os.path.basename(str(path))
            full_path = words_dir / filename
            return str(full_path)
        
        df['file_path'] = df['file_path'].apply(fix_file_path)
        
        # Var olmayan dosyalarÄ± filtrele
        original_size = len(df)
        df = df[df['file_path'].apply(lambda x: x is not None and os.path.exists(x))]
        removed_count = original_size - len(df)
        
        if removed_count > 0:
            print(f"âš ï¸  {removed_count} adet bulunamayan ses dosyasÄ± atlandÄ±.")
        
        if len(df) == 0:
            print(f"âŒ Hata: HiÃ§ geÃ§erli ses dosyasÄ± bulunamadÄ±!")
            return None, None
        
        # SÃ¼tunlarÄ± seÃ§ ve yeniden adlandÄ±r
        df = df[['file_path', 'transcription']].copy()
        df.rename(columns={'transcription': 'transcript'}, inplace=True)
        
        # BoÅŸ transkriptleri filtrele
        df = df[df['transcript'].notna() & (df['transcript'].str.strip() != '')]
        
        if len(df) == 0:
            print(f"âŒ Hata: HiÃ§ geÃ§erli transkript bulunamadÄ±!")
            return None, None
        
        # EÄŸitim ve deÄŸerlendirme setlerine ayÄ±r
        train_df, eval_df = train_test_split(
            df, 
            test_size=test_size, 
            random_state=random_state,
            shuffle=True
        )
        
        # CSV dosyalarÄ±nÄ± kaydet
        train_csv_path = user_path / "train.csv"
        eval_csv_path = user_path / "eval.csv"
        
        train_df.to_csv(train_csv_path, index=False, encoding='utf-8')
        eval_df.to_csv(eval_csv_path, index=False, encoding='utf-8')
        
        print(f"\nâœ… Veri hazÄ±rlama tamamlandÄ±!")
        print(f"   ğŸ“ EÄŸitim seti: {train_csv_path} ({len(train_df)} kayÄ±t)")
        print(f"   ğŸ“ DeÄŸerlendirme seti: {eval_csv_path} ({len(eval_df)} kayÄ±t)")
        print(f"   ğŸ“Š EÄŸitim/DeÄŸerlendirme oranÄ±: {len(train_df)}/{len(eval_df)}")
        
        return train_df, eval_df
        
    except Exception as e:
        print(f"âŒ Hata oluÅŸtu: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def main():
    parser = argparse.ArgumentParser(
        description="KullanÄ±cÄ± verilerini eÄŸitim ve deÄŸerlendirme setlerine ayÄ±rÄ±r."
    )
    parser.add_argument(
        "user_id",
        type=str,
        help="KullanÄ±cÄ± kimliÄŸi (Ã¶rn: Furkan)"
    )
    parser.add_argument(
        "--test_size",
        type=float,
        default=0.2,
        help="DeÄŸerlendirme seti oranÄ± (varsayÄ±lan: 0.2)"
    )
    parser.add_argument(
        "--random_state",
        type=int,
        default=42,
        help="Rastgelelik tohumu (varsayÄ±lan: 42)"
    )
    
    args = parser.parse_args()
    
    prepare_training_data(
        user_id=args.user_id,
        test_size=args.test_size,
        random_state=args.random_state
    )

if __name__ == "__main__":
    main()
