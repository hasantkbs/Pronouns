# Model EÄŸitim KÄ±lavuzu - KonuÅŸma BozukluÄŸu iÃ§in Optimize EdilmiÅŸ

## ğŸ¯ Genel BakÄ±ÅŸ

Bu kÄ±lavuz, konuÅŸma bozukluÄŸu olan bireyler iÃ§in kiÅŸiselleÅŸtirilmiÅŸ ASR modeli eÄŸitimi iÃ§in optimize edilmiÅŸ pipeline'Ä± aÃ§Ä±klar.

## ğŸ“‹ EÄŸitim Ã–ncesi HazÄ±rlÄ±k

### 1. Veri HazÄ±rlama

Furkan kullanÄ±cÄ±sÄ± iÃ§in veri hazÄ±rlama:

```bash
python prepare_training_data.py Furkan
```

Bu komut:
- `metadata_words.csv` dosyasÄ±nÄ± okur
- Veriyi %80 eÄŸitim, %20 deÄŸerlendirme olarak bÃ¶ler
- `train.csv` ve `eval.csv` dosyalarÄ±nÄ± oluÅŸturur

### 2. Veri KontrolÃ¼

EÄŸitim Ã¶ncesi veri kalitesini kontrol edin:
- Ses dosyalarÄ±nÄ±n `data/users/Furkan/words/` klasÃ¶rÃ¼nde olduÄŸundan emin olun
- `metadata_words.csv` dosyasÄ±nÄ±n doÄŸru formatta olduÄŸunu kontrol edin
- Minimum veri miktarÄ±: 100+ kayÄ±t Ã¶nerilir

## ğŸš€ Model EÄŸitimi

### Temel EÄŸitim

```bash
python train_adapter.py Furkan
```

### GeliÅŸmiÅŸ Ã–zellikler

EÄŸitim sÄ±rasÄ±nda ÅŸunlar otomatik olarak yapÄ±lÄ±r:

1. **Veri Augmentation** (KonfigÃ¼rasyonda aktifse):
   - Hafif Gaussian gÃ¼rÃ¼ltÃ¼ ekleme
   - Zaman esnetme (konuÅŸma hÄ±zÄ± varyasyonu)
   - Pitch deÄŸiÅŸimi (hafif)
   - Zaman maskesi

2. **Validation**:
   - Her 50 adÄ±mda bir validation yapÄ±lÄ±r
   - WER (Word Error Rate) ve CER (Character Error Rate) hesaplanÄ±r
   - En iyi model otomatik olarak kaydedilir

3. **Early Stopping**:
   - Validation loss iyileÅŸmezse eÄŸitim durdurulur
   - Overfitting'i Ã¶nler
   - Patience: 5 epoch (config'de ayarlanabilir)

4. **Learning Rate Scheduling**:
   - Warmup: Ä°lk 100 adÄ±mda learning rate kademeli artar
   - SonrasÄ±nda linear decay

## ğŸ“Š EÄŸitim Metrikleri

EÄŸitim sÄ±rasÄ±nda ÅŸu metrikler takip edilir:

- **Training Loss**: Her epoch sonunda gÃ¶sterilir
- **Validation Loss**: Her 50 adÄ±mda bir hesaplanÄ±r
- **WER**: Kelime hata oranÄ± (dÃ¼ÅŸÃ¼k = iyi)
- **CER**: Karakter hata oranÄ± (dÃ¼ÅŸÃ¼k = iyi)

### Ä°yi Performans GÃ¶stergeleri

- WER < 0.15 (%15'ten az kelime hatasÄ±)
- CER < 0.05 (%5'ten az karakter hatasÄ±)
- Validation loss training loss'a yakÄ±n (overfitting yok)

## âš™ï¸ Hyperparameter AyarlarÄ±

### Ã–nerilen Ayarlar (config.py)

```python
# KonuÅŸma bozukluÄŸu iÃ§in optimize edilmiÅŸ
NUM_FINETUNE_EPOCHS = 20          # Daha fazla epoch
FINETUNE_BATCH_SIZE = 4           # Daha bÃ¼yÃ¼k batch
FINETUNE_LEARNING_RATE = 5e-5     # Daha dÃ¼ÅŸÃ¼k LR (stabilite iÃ§in)
ADAPTER_REDUCTION_FACTOR = 16    # Daha fazla parametre
GRADIENT_ACCUMULATION_STEPS = 4   # Efektif batch = 16
WARMUP_STEPS = 100                # Learning rate warmup
EARLY_STOPPING_PATIENCE = 5       # Early stopping
USE_AUGMENTATION = True           # Augmentation aktif
```

### Ayarlama Ä°puÃ§larÄ±

**DÃ¼ÅŸÃ¼k doÄŸruluk durumunda:**
- Epoch sayÄ±sÄ±nÄ± artÄ±rÄ±n (20 â†’ 30)
- Learning rate'i dÃ¼ÅŸÃ¼rÃ¼n (5e-5 â†’ 3e-5)
- Daha fazla veri toplayÄ±n
- Augmentation'Ä± aktif tutun

**Overfitting durumunda:**
- Early stopping patience'Ä± azaltÄ±n (5 â†’ 3)
- Weight decay'i artÄ±rÄ±n (1e-3 â†’ 5e-3)
- Augmentation'Ä± artÄ±rÄ±n
- Daha fazla veri toplayÄ±n

**EÄŸitim Ã§ok yavaÅŸsa:**
- Batch size'Ä± artÄ±rÄ±n (4 â†’ 8)
- Gradient accumulation'Ä± azaltÄ±n (4 â†’ 2)
- Augmentation'Ä± kapatÄ±n (geÃ§ici olarak)

## ğŸ” Model DeÄŸerlendirme

EÄŸitim sonrasÄ± modeli deÄŸerlendirin:

```bash
python evaluate_model.py Furkan
```

Sadece ilk 100 Ã¶rnek iÃ§in:
```bash
python evaluate_model.py Furkan --max_samples 100
```

## ğŸ“ Ã‡Ä±ktÄ± DosyalarÄ±

EÄŸitim sonrasÄ± ÅŸu dosyalar oluÅŸturulur:

```
data/models/personalized_models/Furkan/
â”œâ”€â”€ adapter_config.json          # LoRA adapter konfigÃ¼rasyonu
â”œâ”€â”€ adapter_model.bin            # Adapter aÄŸÄ±rlÄ±klarÄ±
â””â”€â”€ checkpoints/
    â””â”€â”€ best_model/              # En iyi model checkpoint'i
```

## ğŸ› Sorun Giderme

### EÄŸitim sÄ±rasÄ±nda hata

1. **CUDA out of memory**:
   - Batch size'Ä± azaltÄ±n (4 â†’ 2)
   - Gradient accumulation'Ä± artÄ±rÄ±n (4 â†’ 8)

2. **Validation loss artÄ±yor**:
   - Learning rate'i dÃ¼ÅŸÃ¼rÃ¼n
   - Early stopping Ã§alÄ±ÅŸÄ±yor olabilir (normal)

3. **WER/CER Ã§ok yÃ¼ksek**:
   - Daha fazla veri toplayÄ±n
   - Epoch sayÄ±sÄ±nÄ± artÄ±rÄ±n
   - Model checkpoint'lerini kontrol edin

### Veri sorunlarÄ±

1. **Ses dosyalarÄ± bulunamÄ±yor**:
   - `prepare_training_data.py` Ã§alÄ±ÅŸtÄ±rÄ±n
   - Dosya yollarÄ±nÄ± kontrol edin

2. **BoÅŸ transkriptler**:
   - `metadata_words.csv` dosyasÄ±nÄ± kontrol edin
   - BoÅŸ satÄ±rlarÄ± temizleyin

## ğŸ’¡ Ä°puÃ§larÄ±

1. **Ä°lk eÄŸitim**: VarsayÄ±lan ayarlarla baÅŸlayÄ±n
2. **Ä°teratif iyileÅŸtirme**: Her eÄŸitimden sonra deÄŸerlendirme yapÄ±n
3. **Veri kalitesi**: Temiz, net kayÄ±tlar Ã¶nemli
4. **DÃ¼zenli checkpoint**: En iyi model otomatik kaydedilir
5. **Monitoring**: EÄŸitim sÄ±rasÄ±nda metrikleri takip edin

## ğŸ“ˆ Performans Ä°yileÅŸtirme Stratejisi

1. **BaÅŸlangÄ±Ã§**: VarsayÄ±lan ayarlarla eÄŸitin
2. **DeÄŸerlendirme**: WER/CER metriklerini kontrol edin
3. **Ayarlama**: Gerekirse hyperparameter'larÄ± optimize edin
4. **Veri toplama**: DÃ¼ÅŸÃ¼k doÄŸruluk varsa daha fazla veri toplayÄ±n
5. **Tekrar eÄŸitim**: Ä°yileÅŸtirilmiÅŸ ayarlarla tekrar eÄŸitin

## ğŸ“ Kaynaklar

- Wav2Vec2: https://huggingface.co/docs/transformers/model_doc/wav2vec2
- LoRA: https://github.com/microsoft/LoRA
- PEFT: https://github.com/huggingface/peft

