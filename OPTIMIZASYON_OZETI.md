# Model EÄŸitimi Optimizasyon Ã–zeti

## ğŸ¯ YapÄ±lan Ä°yileÅŸtirmeler

Bu dokÃ¼man, konuÅŸma bozukluÄŸu olan bireyler (Furkan) iÃ§in ASR model eÄŸitim pipeline'Ä±nda yapÄ±lan optimizasyonlarÄ± Ã¶zetlemektedir.

## ğŸ“Š Ana DeÄŸiÅŸiklikler

### 1. Veri Augmentation (Veri ZenginleÅŸtirme)

**Ã–nceki Durum:**
- Augmentation yoktu
- SÄ±nÄ±rlÄ± veri Ã§eÅŸitliliÄŸi

**Yeni Durum:**
- KonuÅŸma bozukluÄŸu iÃ§in optimize edilmiÅŸ hafif augmentation
- Gaussian gÃ¼rÃ¼ltÃ¼ ekleme (dÃ¼ÅŸÃ¼k seviye)
- Zaman esnetme (konuÅŸma hÄ±zÄ± varyasyonu)
- Pitch deÄŸiÅŸimi (hafif, -2 ile +2 semitone)
- Zaman maskesi (kÃ¼Ã§Ã¼k bÃ¶lÃ¼mler)
- %60 ihtimalle augmentation uygulanÄ±r (aÅŸÄ±rÄ± distortion'dan kaÃ§Ä±nmak iÃ§in)

**Faydalar:**
- Model genellemesi artar
- Overfitting riski azalÄ±r
- Daha az veri ile daha iyi performans

### 2. Validation ve Early Stopping

**Ã–nceki Durum:**
- EÄŸitim sÄ±rasÄ±nda validation yoktu
- Overfitting riski yÃ¼ksekti
- En iyi modeli seÃ§me mekanizmasÄ± yoktu

**Yeni Durum:**
- Her 50 adÄ±mda bir validation yapÄ±lÄ±r
- WER (Word Error Rate) ve CER (Character Error Rate) hesaplanÄ±r
- Early stopping: Validation loss iyileÅŸmezse eÄŸitim durur
- En iyi model otomatik olarak kaydedilir
- Patience: 5 epoch (config'de ayarlanabilir)

**Faydalar:**
- Overfitting Ã¶nlenir
- En iyi model otomatik seÃ§ilir
- EÄŸitim sÃ¼resi optimize edilir
- GerÃ§ek zamanlÄ± performans takibi

### 3. Hyperparameter Optimizasyonu

**Ã–nceki Ayarlar:**
```python
NUM_FINETUNE_EPOCHS = 15
FINETUNE_BATCH_SIZE = 2
FINETUNE_LEARNING_RATE = 1e-4
ADAPTER_REDUCTION_FACTOR = 32
GRADIENT_ACCUMULATION_STEPS = 2
```

**Yeni Ayarlar (KonuÅŸma BozukluÄŸu iÃ§in Optimize):**
```python
NUM_FINETUNE_EPOCHS = 20              # +5 epoch (daha fazla Ã¶ÄŸrenme)
FINETUNE_BATCH_SIZE = 4               # 2x batch (daha stabil gradient)
FINETUNE_LEARNING_RATE = 5e-5         # 2x dÃ¼ÅŸÃ¼k (daha stabil Ã¶ÄŸrenme)
ADAPTER_REDUCTION_FACTOR = 16         # 2x fazla parametre (daha iyi adaptasyon)
GRADIENT_ACCUMULATION_STEPS = 4       # 2x (efektif batch = 16)
WARMUP_STEPS = 100                    # Yeni: Learning rate warmup
WEIGHT_DECAY = 1e-3                   # Yeni: Overfitting Ã¶nleme
EARLY_STOPPING_PATIENCE = 5           # Yeni: Early stopping
USE_AUGMENTATION = True                # Yeni: Augmentation kontrolÃ¼
```

**Faydalar:**
- Daha stabil eÄŸitim
- Daha iyi adaptasyon (daha fazla parametre)
- Overfitting Ã¶nleme
- Daha iyi genelleme

### 4. LoRA KonfigÃ¼rasyonu Ä°yileÅŸtirmesi

**Ã–nceki Durum:**
- Sadece `q_proj` ve `v_proj` modÃ¼lleri
- SÄ±nÄ±rlÄ± adaptasyon kapasitesi

**Yeni Durum:**
- `q_proj`, `v_proj`, `k_proj`, `out_proj` modÃ¼lleri
- Daha fazla adaptasyon noktasÄ±
- Daha dÃ¼ÅŸÃ¼k dropout (0.05 vs 0.1)
- ASR task type belirtildi

**Faydalar:**
- Daha iyi model adaptasyonu
- KonuÅŸma bozukluÄŸu iÃ§in daha fazla Ã¶ÄŸrenme kapasitesi
- Daha az overfitting riski

### 5. Learning Rate Scheduling

**Ã–nceki Durum:**
- Sabit learning rate
- Warmup yoktu

**Yeni Durum:**
- Linear warmup: Ä°lk 100 adÄ±mda LR kademeli artar
- Linear decay: Warmup sonrasÄ± LR azalÄ±r
- Transformers'Ä±n `get_linear_schedule_with_warmup` kullanÄ±lÄ±yor

**Faydalar:**
- Daha stabil eÄŸitim baÅŸlangÄ±cÄ±
- Daha iyi convergence
- Overfitting riski azalÄ±r

### 6. Gradient Clipping

**Ã–nceki Durum:**
- Gradient clipping yoktu
- Gradient explosion riski

**Yeni Durum:**
- Max norm: 1.0
- Gradient accumulation ile birlikte Ã§alÄ±ÅŸÄ±r

**Faydalar:**
- EÄŸitim stabilitesi
- Gradient explosion Ã¶nlenir
- Daha gÃ¼venilir eÄŸitim

### 7. GeliÅŸmiÅŸ Metrikler ve Logging

**Ã–nceki Durum:**
- Sadece loss gÃ¶steriliyordu
- WER/CER hesaplanmÄ±yordu

**Yeni Durum:**
- Real-time WER ve CER hesaplama
- Validation metrikleri gÃ¶sterimi
- Learning rate takibi
- Progress bar'da detaylÄ± bilgi

**Faydalar:**
- Daha iyi eÄŸitim takibi
- Performans deÄŸerlendirmesi
- Sorun tespiti kolaylaÅŸÄ±r

### 8. Veri Ã–n Ä°ÅŸleme Ä°yileÅŸtirmeleri

**Ã–nceki Durum:**
- Basit filtreleme
- Minimum uzunluk kontrolÃ¼: 100 sample

**Yeni Durum:**
- Minimum uzunluk: 0.1 saniye (1600 sample)
- Daha iyi hata yÃ¶netimi
- Augmentation entegrasyonu
- Train/validation ayrÄ±mÄ±

**Faydalar:**
- Daha kaliteli veri
- Daha az hata
- Daha iyi genelleme

## ğŸ“ˆ Beklenen Ä°yileÅŸtirmeler

### Performans Metrikleri

**Ã–nceki Beklentiler:**
- WER: ~0.20-0.30
- CER: ~0.10-0.15
- Overfitting riski: YÃ¼ksek

**Yeni Beklentiler:**
- WER: <0.15 (hedef)
- CER: <0.05 (hedef)
- Overfitting riski: DÃ¼ÅŸÃ¼k (early stopping ile)

### EÄŸitim SÃ¼resi

- Validation eklenmesi: +%10-20 sÃ¼re
- Early stopping: Ortalama %20-30 zaman tasarrufu
- Augmentation: +%5-10 sÃ¼re

### Model Kalitesi

- Daha iyi genelleme
- Daha stabil eÄŸitim
- En iyi model otomatik seÃ§imi
- Overfitting Ã¶nleme

## ğŸ”§ KullanÄ±m

### Temel EÄŸitim

```bash
# 1. Veri hazÄ±rlama
python prepare_training_data.py Furkan

# 2. Model eÄŸitimi
python train_adapter.py Furkan

# 3. DeÄŸerlendirme
python evaluate_model.py Furkan
```

### KonfigÃ¼rasyon

TÃ¼m ayarlar `config.py` dosyasÄ±nda yapÄ±labilir:

```python
# Augmentation'Ä± kapatmak iÃ§in
USE_AUGMENTATION = False

# Early stopping patience'Ä± artÄ±rmak iÃ§in
EARLY_STOPPING_PATIENCE = 10

# Learning rate'i ayarlamak iÃ§in
FINETUNE_LEARNING_RATE = 3e-5
```

## ğŸ“ Notlar

1. **Ä°lk EÄŸitim**: VarsayÄ±lan ayarlarla baÅŸlayÄ±n
2. **Monitoring**: EÄŸitim sÄ±rasÄ±nda metrikleri takip edin
3. **Iterasyon**: Her eÄŸitimden sonra deÄŸerlendirme yapÄ±n
4. **Veri Kalitesi**: Temiz, net kayÄ±tlar Ã¶nemli
5. **Patience**: Early stopping patience'Ä± veri miktarÄ±na gÃ¶re ayarlayÄ±n

## ğŸ¯ SonuÃ§

Bu optimizasyonlar ile:
- âœ… Daha iyi model performansÄ±
- âœ… Overfitting Ã¶nleme
- âœ… Daha stabil eÄŸitim
- âœ… Otomatik en iyi model seÃ§imi
- âœ… GerÃ§ek zamanlÄ± performans takibi
- âœ… KonuÅŸma bozukluÄŸu iÃ§in Ã¶zelleÅŸtirilmiÅŸ ayarlar

Model eÄŸitimi artÄ±k konuÅŸma bozukluÄŸu olan bireyler iÃ§in optimize edilmiÅŸ durumda!

