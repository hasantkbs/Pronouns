# train_adapter.py
# Konu≈üma bozukluƒüu i√ßin optimize edilmi≈ü model eƒüitim scripti

import os
import sys
import argparse
import torch
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import logging
from transformers import (
    Wav2Vec2ForCTC,
    Wav2Vec2Processor,
)
from peft import LoraConfig, get_peft_model
from torch.utils.data import DataLoader
from torch.optim import AdamW
from accelerate import Accelerator
from tqdm import tqdm
from datasets import Dataset, Audio
from src.utils.utils import save_model_and_processor
from src.services.reporting_service import ReportingService
import config
import librosa
import evaluate
import platform
import time
try:
    import audiomentations as A
    AUDIOMENTATIONS_AVAILABLE = True
except ImportError:
    AUDIOMENTATIONS_AVAILABLE = False
    print("‚ö†Ô∏è  audiomentations bulunamadƒ±. Augmentation kullanƒ±lamayacak.")

# Linux sunucu i√ßin logging yapƒ±landƒ±rmasƒ±
def setup_logging(user_id):
    """Linux sunucu i√ßin logging yapƒ±landƒ±rmasƒ±."""
    log_dir = Path(config.LOG_DIR)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    log_file = log_dir / f"training_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # Logging formatƒ±
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    
    # Log seviyesi
    log_level = getattr(logging, config.LOG_LEVEL.upper(), logging.INFO)
    
    # File handler
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(log_level)
    file_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(log_level)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # Root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    print(f"üìù Log dosyasƒ±: {log_file}")
    return log_file

class DataCollatorCTCWithPadding:
    """
    CTC tabanlƒ± ASR i√ßin data collator.
    Wav2Vec2 i√ßin input_values ve labels ayrƒ± ayrƒ± pad edilir.
    """
    def __init__(self, processor):
        self.processor = processor

    def __call__(self, features):
        # input_values i√ßin padding
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        batch = self.processor.feature_extractor.pad(
            input_features,
            padding=True,
            return_tensors="pt",
        )
        
        # labels i√ßin padding
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(
            label_features,
            padding=True,
            return_tensors="pt",
        )
        
        # Padding token'larƒ± -100'e √ßevir (CTC loss i√ßin)
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch["attention_mask"].ne(1), 
            -100
        )
        batch["labels"] = labels
        return batch

def build_augment_pipeline(sampling_rate: int):
    """
    Konu≈üma bozukluƒüu i√ßin optimize edilmi≈ü augmentation pipeline.
    Hafif augmentation kullanƒ±r (a≈üƒ±rƒ± distortion'dan ka√ßƒ±nƒ±r).
    """
    if not AUDIOMENTATIONS_AVAILABLE:
        return None
    
    return A.Compose([
        # Hafif g√ºr√ºlt√º ekleme (konu≈üma bozukluƒüu i√ßin d√º≈ü√ºk seviye)
        A.AddGaussianNoise(min_amplitude=0.0005, max_amplitude=0.005, p=0.3),
        # Zaman esnetme (konu≈üma hƒ±zƒ± varyasyonu)
        A.TimeStretch(min_rate=0.9, max_rate=1.1, p=0.3, leave_length_unchanged=False),
        # Pitch deƒüi≈üimi (hafif, konu≈üma bozukluƒüu i√ßin)
        A.PitchShift(min_semitones=-2, max_semitones=2, p=0.3),
        # Zaman maskesi (k√º√ß√ºk b√∂l√ºmler)
        A.TimeMask(min_band_part=0.03, max_band_part=0.1, p=0.2),
    ], p=0.6)  # %60 ihtimalle augmentation uygula

def _standalone_preprocess_function(examples, processor, augmenter=None):
    """
    Standalone data preprocessing function for multiprocessing.
    Wav2Vec2 i√ßin √∂zellik √ßƒ±karƒ±mƒ± ve tokenization yapar.
    Augmentation desteƒüi eklenmi≈ütir.
    """
    # Ses dosyalarƒ±nƒ± y√ºkle
    audio_arrays = []
    valid_transcripts = []
    
    # Transcript s√ºtununu belirle
    transcript_key = "transcript" if "transcript" in examples else "transcription"
    transcripts = examples.get(transcript_key, [""] * len(examples["file_path"]))
    
    for i, path_dict in enumerate(examples["file_path"]):
        try:
            audio, sr = librosa.load(path_dict['path'], sr=config.ORNEKLEME_ORANI)
            
            # Minimum uzunluk kontrol√º (en az 0.1 saniye)
            if len(audio) < config.ORNEKLEME_ORANI * 0.1:
                continue
            
            # Augmentation uygula (eƒüer varsa)
            if augmenter is not None:
                try:
                    audio = augmenter(samples=audio, sample_rate=sr)
                except Exception as e:
                    # Augmentation hatasƒ± durumunda orijinal sesi kullan
                    pass
            
            # Transcript kontrol√º
            transcript = str(transcripts[i]).strip() if i < len(transcripts) else ""
            if transcript:
                audio_arrays.append(audio)
                valid_transcripts.append(transcript)
        except Exception as e:
            # Hata durumunda sessizce atla
            continue
    
    if len(audio_arrays) == 0:
        # Bo≈ü batch i√ßin dummy deƒüerler d√∂nd√ºr
        return {
            "input_values": np.array([0.0]),
            "labels": [processor.tokenizer.pad_token_id]
        }
    
    # Processor ile √∂zellik √ßƒ±karƒ±mƒ± (her √∂rnek i√ßin ayrƒ± ayrƒ±)
    input_values_list = []
    label_ids_list = []
    valid_indices = []
    
    for i, (audio, transcript) in enumerate(zip(audio_arrays, valid_transcripts)):
        try:
            # Her √∂rnek i√ßin ayrƒ± ayrƒ± i≈üle
            inputs = processor(
                audio, 
                sampling_rate=config.ORNEKLEME_ORANI, 
                return_tensors="pt", 
                padding=False  # Padding yapma, collator yapacak
            )
            
            # Input values'ƒ± list'e √ßevir
            input_vals = inputs.input_values[0]
            if isinstance(input_vals, torch.Tensor):
                input_vals = input_vals.tolist()
            input_values_list.append(input_vals)
            
            # Transcript'i tokenize et
            # tokenizer() dict d√∂nd√ºr√ºr, input_ids'i al
            tokenized = processor.tokenizer(transcript)
            label_ids = tokenized.input_ids
            
            # Format kontrol√º ve d√ºzeltme
            if isinstance(label_ids, torch.Tensor):
                label_ids = label_ids.tolist()
            
            # Eƒüer nested list ise ([[1,2,3]] gibi) flatten et
            if label_ids and isinstance(label_ids[0], list):
                label_ids = label_ids[0]
            
            # Bo≈ü label kontrol√º
            if not label_ids or len(label_ids) == 0:
                print(f"‚ö†Ô∏è  Bo≈ü label: '{transcript}' -> atlanƒ±yor")
                continue
            
            # Label'larƒ±n ge√ßerli token ID'leri i√ßerdiƒüini kontrol et
            if any(not isinstance(tid, int) or tid < 0 for tid in label_ids):
                print(f"‚ö†Ô∏è  Ge√ßersiz label IDs: {label_ids[:5]}... -> atlanƒ±yor")
                continue
                
            label_ids_list.append(label_ids)
            valid_indices.append(i)
            
        except Exception as e:
            print(f"‚ö†Ô∏è  √ñrnek {i} i≈ülenirken hata: {e}")
            continue
    
    # Eƒüer hi√ß ge√ßerli √∂rnek yoksa
    if len(input_values_list) == 0:
        return {
            "input_values": [[0.0]],
            "labels": [[processor.tokenizer.pad_token_id]]
        }
    
    # Sonu√ßlarƒ± dict olarak d√∂nd√ºr (collator i√ßin)
    result = {
        "input_values": input_values_list,
        "labels": label_ids_list
    }
    
    return result

class PersonalizedTrainer:
    def __init__(self, user_id, base_model_path=None):
        self.user_id = user_id
        self.base_model_path = base_model_path or config.MODEL_NAME
        self.user_data_path = Path(config.BASE_PATH) / self.user_id
        self.output_dir = Path("data/models/personalized_models") / self.user_id
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processor = None
        self.model = None
        self.wer_metric = evaluate.load("wer")
        self.cer_metric = evaluate.load("cer")
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.checkpoint_dir = self.output_dir / "checkpoints"

    def run(self):
        # Linux sunucu i√ßin logging ba≈ülat
        log_file = setup_logging(self.user_id)
        logger = logging.getLogger(__name__)
        
        # Reporting service
        reporting_service = ReportingService()
        
        # Training start time
        training_start_time = time.time()
        start_datetime = datetime.now()
        
        print(f"üéØ {self.user_id} i√ßin ki≈üiselle≈ütirme s√ºreci ba≈ülƒ±yor.")
        print("="*50)
        logger.info(f"Training started for user: {self.user_id}")
        
        # System information
        sys_info = {
            "platform": platform.system(),
            "platform_release": platform.release(),
            "python_version": sys.version.split()[0],  # Just version number
            "cuda_available": torch.cuda.is_available(),
            "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
            "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            "gpu_memory_gb": round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2) if torch.cuda.is_available() else None
        }
        
        logger.info(f"Platform: {sys_info['platform']} {sys_info['platform_release']}")
        logger.info(f"Python: {sys_info['python_version']}")
        if torch.cuda.is_available():
            logger.info(f"CUDA Version: {sys_info['cuda_version']}")
            logger.info(f"GPU: {sys_info['gpu_name']}")
            logger.info(f"GPU Memory: {sys_info['gpu_memory_gb']:.2f} GB")
        
        if not self.user_data_path.exists() or not (self.user_data_path / "metadata_words.csv").exists():
            error_msg = f"‚ùå Hata: {self.user_data_path} i√ßin veri bulunamadƒ±."
            print(error_msg)
            logger.error(error_msg)
            return

        self.load_model_and_processor()
        train_dataset = self.prepare_dataset(split='train')
        eval_dataset = self.prepare_dataset(split='eval')
        
        # Store dataset info for reporting
        self.train_samples = len(train_dataset) if train_dataset else 0
        self.eval_samples = len(eval_dataset) if eval_dataset else 0
        
        # Train model (will store metrics in self)
        self.train_model(train_dataset, eval_dataset, reporting_service, sys_info, start_datetime, training_start_time)
        
        logger.info(f"Training completed for user: {self.user_id}")

    def load_model_and_processor(self):
        print(f"üì• Temel model y√ºkleniyor: {self.base_model_path}")
        self.processor = Wav2Vec2Processor.from_pretrained(self.base_model_path)
        self.model = Wav2Vec2ForCTC.from_pretrained(self.base_model_path)
        
        # Gradient checkpointing (opsiyonel, VRAM tasarrufu i√ßin)
        if config.GRADIENT_CHECKPOINTING and hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
            print("   ‚úÖ Gradient checkpointing aktif (VRAM tasarrufu)")
        
        self.model.to(self.device)
        
        # Konu≈üma bozukluƒüu i√ßin optimize edilmi≈ü LoRA konfig√ºrasyonu
        # Daha fazla mod√ºl ve daha y√ºksek rank kullanƒ±yoruz
        # Not: PEFT otomatik olarak Wav2Vec2ForCTC model tipini algƒ±lar, task_type gerekmez
        peft_config = LoraConfig(
            r=config.ADAPTER_REDUCTION_FACTOR,
            lora_alpha=config.ADAPTER_REDUCTION_FACTOR * 2,
            target_modules=["q_proj", "v_proj", "k_proj", "out_proj"],  # Daha fazla mod√ºl
            lora_dropout=0.05,  # Daha d√º≈ü√ºk dropout (overfitting riski d√º≈ü√ºk)
            bias="none",
            # task_type parametresi kaldƒ±rƒ±ldƒ± - PEFT otomatik algƒ±lar
        )
        self.model = get_peft_model(self.model, peft_config)
        
        # Trainable parametreleri g√∂ster
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"‚úÖ Model PEFT/LoRA ile sarmalandƒ±. Cihaz: {self.device}")
        print(f"   Eƒüitilebilir parametreler: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)")

    def prepare_dataset(self, split='train'):
        """
        Veri setini hazƒ±rlar ve y√ºkler.
        
        Args:
            split: 'train' veya 'eval'
        """
        print(f"üìä {split.upper()} veri seti hazƒ±rlanƒ±yor: {self.user_data_path}")
        
        # Split'e g√∂re dosya se√ß
        if split == 'train':
            train_csv = self.user_data_path / "train.csv"
            if train_csv.exists():
                print(f"   ‚úÖ train.csv bulundu, kullanƒ±lƒ±yor.")
                df = pd.read_csv(train_csv, encoding='utf-8')
            else:
                # metadata_words.csv'den olu≈ütur
                metadata_path = self.user_data_path / "metadata_words.csv"
                if not metadata_path.exists():
                    raise FileNotFoundError(
                        f"‚ùå Hata: Ne train.csv ne de metadata_words.csv bulunamadƒ±!\n"
                        f"   L√ºtfen √∂nce 'python prepare_training_data.py {self.user_id}' √ßalƒ±≈ütƒ±rƒ±n."
                    )
                
                print(f"   ‚ö†Ô∏è  train.csv bulunamadƒ±, metadata_words.csv kullanƒ±lƒ±yor.")
                df = pd.read_csv(metadata_path, encoding='utf-8')
                df = df[['file_path', 'transcription']].copy()
                df.rename(columns={'transcription': 'transcript'}, inplace=True)
        else:  # eval
            eval_csv = self.user_data_path / "eval.csv"
            if eval_csv.exists():
                print(f"   ‚úÖ eval.csv bulundu, kullanƒ±lƒ±yor.")
                df = pd.read_csv(eval_csv, encoding='utf-8')
            else:
                # metadata_words.csv'den olu≈ütur (validation i√ßin)
                metadata_path = self.user_data_path / "metadata_words.csv"
                if not metadata_path.exists():
                    print(f"   ‚ö†Ô∏è  eval.csv bulunamadƒ±, validation seti olu≈üturulamƒ±yor.")
                    return None
                
                print(f"   ‚ö†Ô∏è  eval.csv bulunamadƒ±, metadata_words.csv'nin %20'si validation i√ßin kullanƒ±lƒ±yor.")
                df = pd.read_csv(metadata_path, encoding='utf-8')
                df = df[['file_path', 'transcription']].copy()
                df.rename(columns={'transcription': 'transcript'}, inplace=True)
                # Son %20'yi validation i√ßin al
                df = df.tail(int(len(df) * 0.2))

        # Dosya yollarƒ±nƒ± d√ºzelt
        words_dir = self.user_data_path / "words"
        def fix_file_path(path):
            filename = os.path.basename(str(path))
            return str(words_dir / filename)
        
        df["file_path"] = df["file_path"].apply(fix_file_path)
        
        # Var olmayan dosyalarƒ± filtrele
        original_size = len(df)
        df = df[df["file_path"].apply(os.path.exists)]
        if len(df) < original_size:
            print(f"   ‚ö†Ô∏è  {original_size - len(df)} adet bulunamayan ses dosyasƒ± atlandƒ±.")
        
        if len(df) == 0:
            if split == 'eval':
                return None
            raise ValueError(f"‚ùå Hata: Hi√ß ge√ßerli ses dosyasƒ± bulunamadƒ±!")
        
        # Bo≈ü transkriptleri filtrele
        df = df[df['transcript'].notna() & (df['transcript'].str.strip() != '')]
        
        dataset = Dataset.from_pandas(df)
        dataset = dataset.cast_column("file_path", Audio(sampling_rate=config.ORNEKLEME_ORANI, decode=False))
        
        print(f"   üìà {split.upper()} veri seti boyutu: {len(dataset)} kayƒ±t")
        return dataset

    def evaluate_model(self, eval_dataloader, accelerator):
        """Validation setinde modeli deƒüerlendirir."""
        self.model.eval()
        total_loss = 0.0
        all_predictions = []
        all_references = []
        num_batches = 0
        
        with torch.no_grad():
            for batch in eval_dataloader:
                outputs = self.model(**batch)
                loss = outputs.loss
                total_loss += loss.item()
                num_batches += 1
                
                # WER/CER hesaplama i√ßin tahminler
                logits = outputs.logits
                predicted_ids = torch.argmax(logits, dim=-1)
                predictions = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)
                
                # Referans metinleri
                label_ids = batch["labels"]
                label_ids[label_ids == -100] = self.processor.tokenizer.pad_token_id
                references = self.processor.batch_decode(label_ids, skip_special_tokens=True)
                
                all_predictions.extend(predictions)
                all_references.extend(references)
        
        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
        
        # WER ve CER hesapla
        wer = self.wer_metric.compute(predictions=all_predictions, references=all_references)
        cer = self.cer_metric.compute(predictions=all_predictions, references=all_references)
        
        self.model.train()
        return avg_loss, wer, cer

    def train_model(self, train_dataset, eval_dataset=None, reporting_service=None, 
                   sys_info=None, start_datetime=None, training_start_time=None):
        """Model eƒüitimini ba≈ülatƒ±r."""
        print("üöÄ Ki≈üiselle≈ütirilmi≈ü model eƒüitimi ba≈ülƒ±yor...")
        print(f"   Epoch sayƒ±sƒ±: {config.NUM_FINETUNE_EPOCHS}")
        print(f"   Batch size: {config.FINETUNE_BATCH_SIZE}")
        print(f"   Learning rate: {config.FINETUNE_LEARNING_RATE}")
        print(f"   Gradient accumulation: {config.GRADIENT_ACCUMULATION_STEPS}")
        print(f"   Augmentation: {'Aktif' if config.USE_AUGMENTATION and AUDIOMENTATIONS_AVAILABLE else 'Pasif'}")
        print(f"   Validation: {'Aktif' if eval_dataset is not None else 'Pasif'}")
        
        # Initialize training metrics
        self.final_train_loss = None
        self.final_wer = None
        self.final_cer = None
        self.best_wer = float('inf')
        self.best_cer = float('inf')
        self.epochs_completed = 0
        self.early_stopped = False

        # Augmentation pipeline olu≈ütur
        augmenter = None
        if config.USE_AUGMENTATION and AUDIOMENTATIONS_AVAILABLE:
            augmenter = build_augment_pipeline(config.ORNEKLEME_ORANI)

        # Veri √∂n i≈üleme (sistem kaynaklarƒ±na g√∂re optimize)
        num_proc = min(config.DATA_PREPROCESSING_NUM_PROC, os.cpu_count() or 1)
        print(f"\n‚öôÔ∏è  Veri √∂n i≈üleme {num_proc} CPU √ßekirdeƒüi ile paralelle≈ütiriliyor...")
        
        try:
            # Training set preprocessing (with augmentation)
            processed_train_dataset = train_dataset.map(
                _standalone_preprocess_function,
                fn_kwargs={"processor": self.processor, "augmenter": augmenter},
                remove_columns=train_dataset.column_names,
                batched=True,
                batch_size=config.FINETUNE_BATCH_SIZE,
                num_proc=num_proc
            )
            
            # Bo≈ü √∂rnekleri filtrele
            processed_train_dataset = processed_train_dataset.filter(
                lambda x: len(x.get("input_values", [])) > 0 and len(x.get("labels", [])) > 0
            )
            
            if len(processed_train_dataset) == 0:
                raise ValueError("‚ùå Hata: √ñn i≈üleme sonrasƒ± hi√ß ge√ßerli √∂rnek kalmadƒ±!")
            
            print(f"   ‚úÖ Training set √∂n i≈üleme tamamlandƒ±. {len(processed_train_dataset)} ge√ßerli √∂rnek.")
            
            # Validation set preprocessing (no augmentation)
            processed_eval_dataset = None
            eval_dataloader = None
            if eval_dataset is not None:
                processed_eval_dataset = eval_dataset.map(
                    _standalone_preprocess_function,
                    fn_kwargs={"processor": self.processor, "augmenter": None},
                    remove_columns=eval_dataset.column_names,
                    batched=True,
                    batch_size=config.FINETUNE_BATCH_SIZE,
                    num_proc=num_proc
                )
                
                processed_eval_dataset = processed_eval_dataset.filter(
                    lambda x: len(x.get("input_values", [])) > 0 and len(x.get("labels", [])) > 0
                )
                
                if len(processed_eval_dataset) > 0:
                    print(f"   ‚úÖ Validation set √∂n i≈üleme tamamlandƒ±. {len(processed_eval_dataset)} ge√ßerli √∂rnek.")
                else:
                    print(f"   ‚ö†Ô∏è  Validation seti bo≈ü, validation atlanacak.")
                    processed_eval_dataset = None
            
        except Exception as e:
            print(f"‚ùå Veri √∂n i≈üleme hatasƒ±: {e}")
            import traceback
            traceback.print_exc()
            return

        # Data collator ve dataloader
        data_collator = DataCollatorCTCWithPadding(processor=self.processor)

        # RTX A5000 i√ßin optimize edilmi≈ü DataLoader ayarlarƒ±
        train_dataloader = DataLoader(
            processed_train_dataset,
            batch_size=config.FINETUNE_BATCH_SIZE,
            collate_fn=data_collator,
            shuffle=True,
            num_workers=config.DATALOADER_NUM_WORKERS,
            pin_memory=config.DATALOADER_PIN_MEMORY,
            prefetch_factor=config.DATALOADER_PREFETCH_FACTOR if config.DATALOADER_NUM_WORKERS > 0 else None,
            persistent_workers=True if config.DATALOADER_NUM_WORKERS > 0 else False
        )
        
        if processed_eval_dataset is not None:
            eval_dataloader = DataLoader(
                processed_eval_dataset,
                batch_size=config.FINETUNE_BATCH_SIZE,
                collate_fn=data_collator,
                shuffle=False,
                num_workers=config.DATALOADER_NUM_WORKERS,
                pin_memory=config.DATALOADER_PIN_MEMORY,
                prefetch_factor=config.DATALOADER_PREFETCH_FACTOR if config.DATALOADER_NUM_WORKERS > 0 else None,
                persistent_workers=True if config.DATALOADER_NUM_WORKERS > 0 else False
            )

        # Optimizer with warmup
        optimizer = AdamW(
            self.model.parameters(), 
            lr=config.FINETUNE_LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY
        )
        
        # Learning rate scheduler (linear warmup)
        num_training_steps = config.NUM_FINETUNE_EPOCHS * len(train_dataloader)
        warmup_steps = config.WARMUP_STEPS
        from transformers import get_linear_schedule_with_warmup
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=num_training_steps
        )
        
        # Accelerator (RTX A5000 i√ßin optimize edilmi≈ü ayarlar)
        mixed_precision_mode = config.MIXED_PRECISION if torch.cuda.is_available() else "no"
        if mixed_precision_mode == "fp16" and not torch.cuda.is_available():
            print("‚ö†Ô∏è  FP16 se√ßildi ancak CUDA yok, mixed precision kapatƒ±lƒ±yor.")
            mixed_precision_mode = "no"
        
        accelerator = Accelerator(
            mixed_precision=mixed_precision_mode,
            gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS
        )
        
        if torch.cuda.is_available():
            print(f"‚úÖ GPU kullanƒ±lƒ±yor: {torch.cuda.get_device_name(0)}")
            print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
            print(f"   Mixed Precision: {mixed_precision_mode}")
            print(f"   Batch Size: {config.FINETUNE_BATCH_SIZE}")
            print(f"   Gradient Accumulation: {config.GRADIENT_ACCUMULATION_STEPS}")
            print(f"   Effective Batch Size: {config.FINETUNE_BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}")
        else:
            print("‚ö†Ô∏è  CUDA yok, CPU kullanƒ±lƒ±yor.")
        
        self.model, optimizer, train_dataloader, scheduler = accelerator.prepare(
            self.model, optimizer, train_dataloader, scheduler
        )
        
        if eval_dataloader is not None:
            eval_dataloader = accelerator.prepare(eval_dataloader)

        # Eƒüitim d√∂ng√ºs√º
        num_epochs = config.NUM_FINETUNE_EPOCHS
        progress_bar = tqdm(range(num_training_steps), desc="Eƒüitim")
        
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            global_step = 0
            for epoch in range(num_epochs):
                epoch_loss = 0.0
                num_batches = 0
                
                self.model.train()
                for step, batch in enumerate(train_dataloader):
                    with accelerator.accumulate(self.model):
                        outputs = self.model(**batch)
                        loss = outputs.loss
                        
                        # Loss kontrol√º (negatif veya invalid loss i√ßin)
                        if torch.isnan(loss) or torch.isinf(loss):
                            print(f"‚ö†Ô∏è  Epoch {epoch+1}, Step {step}: Ge√ßersiz loss ({loss.item()}), batch atlanƒ±yor.")
                            continue
                        
                        if loss.item() < 0:
                            print(f"‚ö†Ô∏è  Epoch {epoch+1}, Step {step}: Negatif loss ({loss.item()}), batch atlanƒ±yor.")
                            # Debug i√ßin batch bilgilerini yazdƒ±r (sadece ilk batch'te)
                            if step == 0 and epoch == 0:
                                print(f"   Debug - Batch keys: {list(batch.keys())}")
                                print(f"   Debug - Input shape: {batch['input_values'].shape}")
                                print(f"   Debug - Labels shape: {batch['labels'].shape}")
                                print(f"   Debug - Labels min/max: {batch['labels'].min().item()}/{batch['labels'].max().item()}")
                                print(f"   Debug - Labels sample (first 10): {batch['labels'][0][:10].tolist()}")
                                # ƒ∞lk birka√ß label'ƒ± decode et
                                try:
                                    sample_labels = batch['labels'][0].clone()
                                    sample_labels[sample_labels == -100] = self.processor.tokenizer.pad_token_id
                                    decoded = self.processor.tokenizer.decode(sample_labels[:20], skip_special_tokens=False)
                                    print(f"   Debug - Decoded sample: {decoded}")
                                except Exception as e:
                                    print(f"   Debug - Decode hatasƒ±: {e}")
                            continue
                        
                        accelerator.backward(loss)
                        
                        # Gradient clipping (stabilite i√ßin)
                        if accelerator.sync_gradients:
                            accelerator.clip_grad_norm_(self.model.parameters(), max_norm=config.MAX_GRAD_NORM)
                        
                        optimizer.step()
                        scheduler.step()
                        optimizer.zero_grad()
                    
                    epoch_loss += loss.item()
                    num_batches += 1
                    global_step += 1
                    
                    progress_bar.update(1)
                    avg_loss = epoch_loss / num_batches
                    current_lr = optimizer.param_groups[0]['lr']
                    progress_bar.set_description(
                        f"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | LR: {current_lr:.2e}"
                    )
                    
                    # Validation (belirli adƒ±mlarda)
                    if eval_dataloader is not None and global_step % config.FINETUNE_EVAL_STEPS == 0:
                        val_loss, wer, cer = self.evaluate_model(eval_dataloader, accelerator)
                        print(f"\n   üìä Validation (Step {global_step}):")
                        print(f"      Loss: {val_loss:.4f} | WER: {wer:.4f} ({wer*100:.2f}%) | CER: {cer:.4f} ({cer*100:.2f}%)")
                        
                        # Early stopping kontrol√º
                        if val_loss < self.best_val_loss:
                            self.best_val_loss = val_loss
                            self.patience_counter = 0
                            # Best model'i kaydet
                            unwrapped_model = accelerator.unwrap_model(self.model)
                            best_checkpoint = self.checkpoint_dir / "best_model"
                            save_model_and_processor(unwrapped_model, self.processor, str(best_checkpoint))
                            print(f"      ‚úÖ Yeni en iyi model kaydedildi! (Loss: {val_loss:.4f})")
                        else:
                            self.patience_counter += 1
                            if self.patience_counter >= config.EARLY_STOPPING_PATIENCE:
                                print(f"\n   ‚èπÔ∏è  Early stopping tetiklendi! (Patience: {config.EARLY_STOPPING_PATIENCE})")
                                print(f"      En iyi validation loss: {self.best_val_loss:.4f}")
                                self.early_stopped = True
                                break
                
                avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0.0
                self.final_train_loss = avg_epoch_loss
                self.epochs_completed = epoch + 1
                print(f"\n   Epoch {epoch+1}/{num_epochs} tamamlandƒ±. Ortalama Loss: {avg_epoch_loss:.4f}")
                
                # Epoch sonunda validation
                if eval_dataloader is not None:
                    val_loss, wer, cer = self.evaluate_model(eval_dataloader, accelerator)
                    print(f"   üìä Epoch sonu Validation:")
                    print(f"      Loss: {val_loss:.4f} | WER: {wer:.4f} ({wer*100:.2f}%) | CER: {cer:.4f} ({cer*100:.2f}%)")
                    
                    # Track best metrics
                    if wer < self.best_wer:
                        self.best_wer = wer
                    if cer < self.best_cer:
                        self.best_cer = cer
                
                # Early stopping kontrol√º
                if self.patience_counter >= config.EARLY_STOPPING_PATIENCE:
                    break

            print("\n‚úÖ Model ince ayarƒ± tamamlandƒ±!")
            
            # En iyi modeli y√ºkle ve kaydet
            if (self.checkpoint_dir / "best_model").exists():
                print(f"   üì• En iyi model y√ºkleniyor...")
                from peft import PeftModel
                base_model = Wav2Vec2ForCTC.from_pretrained(self.base_model_path)
                best_model = PeftModel.from_pretrained(base_model, str(self.checkpoint_dir / "best_model"))
                unwrapped_model = accelerator.unwrap_model(best_model)
            else:
                unwrapped_model = accelerator.unwrap_model(self.model)
            
            self.output_dir.mkdir(parents=True, exist_ok=True)
            save_model_and_processor(unwrapped_model, self.processor, str(self.output_dir))

            print(f"üíæ Ki≈üiselle≈ütirilmi≈ü model kaydedildi: {self.output_dir}")
            print(f"   Toplam eƒüitim adƒ±mƒ±: {global_step}")
            if eval_dataloader is not None:
                final_val_loss, final_wer, final_cer = self.evaluate_model(eval_dataloader, accelerator)
                self.final_wer = final_wer
                self.final_cer = final_cer
                print(f"   Final Validation - Loss: {final_val_loss:.4f} | WER: {final_wer:.4f} ({final_wer*100:.2f}%) | CER: {final_cer:.4f} ({final_cer*100:.2f}%)")
            
            # Create training report
            if reporting_service:
                training_end_time = time.time()
                duration_seconds = training_end_time - training_start_time
                hours = int(duration_seconds // 3600)
                minutes = int((duration_seconds % 3600) // 60)
                seconds = int(duration_seconds % 60)
                duration_formatted = f"{hours:02d}:{minutes:02d}:{seconds:02d}"
                
                training_data = {
                    "base_model": self.base_model_path,
                    "num_epochs": config.NUM_FINETUNE_EPOCHS,
                    "batch_size": config.FINETUNE_BATCH_SIZE,
                    "learning_rate": config.FINETUNE_LEARNING_RATE,
                    "gradient_accumulation_steps": config.GRADIENT_ACCUMULATION_STEPS,
                    "adapter_reduction_factor": config.ADAPTER_REDUCTION_FACTOR,
                    "warmup_steps": config.WARMUP_STEPS,
                    "weight_decay": config.WEIGHT_DECAY,
                    "early_stopping_patience": config.EARLY_STOPPING_PATIENCE,
                    "use_augmentation": config.USE_AUGMENTATION and AUDIOMENTATIONS_AVAILABLE,
                    "mixed_precision": getattr(config, "MIXED_PRECISION", "no"),
                    "train_samples": self.train_samples,
                    "eval_samples": self.eval_samples,
                    "total_samples": self.train_samples + self.eval_samples,
                    "total_steps": global_step,
                    "epochs_completed": self.epochs_completed,
                    "final_train_loss": self.final_train_loss,
                    "best_val_loss": self.best_val_loss,
                    "final_wer": self.final_wer,
                    "final_cer": self.final_cer,
                    "best_wer": self.best_wer if self.best_wer != float('inf') else None,
                    "best_cer": self.best_cer if self.best_cer != float('inf') else None,
                    "early_stopped": self.early_stopped,
                    "device": self.device,
                    "gpu_name": sys_info.get("gpu_name") if sys_info else None,
                    "gpu_memory_gb": sys_info.get("gpu_memory_gb") if sys_info else None,
                    "platform": sys_info.get("platform") if sys_info else None,
                    "python_version": sys_info.get("python_version") if sys_info else None,
                    "start_time": start_datetime.isoformat() if start_datetime else None,
                    "end_time": datetime.now().isoformat(),
                    "duration_seconds": duration_seconds,
                    "duration_formatted": duration_formatted
                }
                
                report_file = reporting_service.log_training_session(self.user_id, training_data)
                print(f"\nüìä Training report saved: {report_file}")
            
        except Exception as e:
            print(f"\n‚ùå Eƒüitim sƒ±rasƒ±nda hata olu≈ütu: {e}")
            import traceback
            traceback.print_exc()
            raise

def main():
    parser = argparse.ArgumentParser(description="Kullanƒ±cƒ±ya √∂zel ASR modelini eƒüitir.")
    parser.add_argument("user_id", type=str, help="Verisi kullanƒ±lacak ve modeli ki≈üiselle≈ütirilecek kullanƒ±cƒ±nƒ±n kimliƒüi.")
    parser.add_argument("--base_model", type=str, help="ƒ∞nce ayar i√ßin kullanƒ±lacak temel modelin yolu. Varsayƒ±lan: config.py'deki model.", default=None)
    
    args = parser.parse_args()
    
    trainer = PersonalizedTrainer(user_id=args.user_id, base_model_path=args.base_model)
    trainer.run()

if __name__ == "__main__":
    import platform
    import sys
    
    # Linux sunucu i√ßin multiprocessing optimizasyonu
    if platform.system() == "Linux":
        import multiprocessing as mp
        try:
            # Linux'ta fork daha hƒ±zlƒ± ve verimli
            mp.set_start_method(config.MULTIPROCESSING_START_METHOD, force=True)
            print(f"‚úÖ Linux sunucu: Multiprocessing start method = {config.MULTIPROCESSING_START_METHOD}")
        except RuntimeError:
            # Zaten ayarlanmƒ±≈üsa devam et
            pass
    else:
        # Windows/Mac i√ßin spawn
        if torch.cuda.is_available():
            try:
                import multiprocessing as mp
                mp.set_start_method("spawn", force=True)
            except RuntimeError:
                pass
    
    # CUDA device se√ßimi (Linux sunucuda birden fazla GPU varsa)
    if torch.cuda.is_available() and config.CUDA_VISIBLE_DEVICES is not None:
        os.environ["CUDA_VISIBLE_DEVICES"] = str(config.CUDA_VISIBLE_DEVICES)
        print(f"‚úÖ CUDA_VISIBLE_DEVICES = {config.CUDA_VISIBLE_DEVICES}")

    main()