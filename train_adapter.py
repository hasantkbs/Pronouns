# train_adapter.py
# KonuÅŸma bozukluÄŸu iÃ§in optimize edilmiÅŸ model eÄŸitim scripti

import os
import sys
import argparse
import torch
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import logging
from transformers import (
    Wav2Vec2ForCTC,
    Wav2Vec2Processor,
)
from peft import LoraConfig, get_peft_model
from torch.utils.data import DataLoader
from torch.optim import AdamW
from accelerate import Accelerator
from tqdm import tqdm
from datasets import Dataset, Audio
from src.utils.utils import save_model_and_processor
import config
import librosa
import evaluate
try:
    import audiomentations as A
    AUDIOMENTATIONS_AVAILABLE = True
except ImportError:
    AUDIOMENTATIONS_AVAILABLE = False
    print("âš ï¸  audiomentations bulunamadÄ±. Augmentation kullanÄ±lamayacak.")

# Linux sunucu iÃ§in logging yapÄ±landÄ±rmasÄ±
def setup_logging(user_id):
    """Linux sunucu iÃ§in logging yapÄ±landÄ±rmasÄ±."""
    log_dir = Path(config.LOG_DIR)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    log_file = log_dir / f"training_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # Logging formatÄ±
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    
    # Log seviyesi
    log_level = getattr(logging, config.LOG_LEVEL.upper(), logging.INFO)
    
    # File handler
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(log_level)
    file_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(log_level)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # Root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    print(f"ğŸ“ Log dosyasÄ±: {log_file}")
    return log_file

class DataCollatorCTCWithPadding:
    """
    CTC tabanlÄ± ASR iÃ§in data collator.
    Wav2Vec2 iÃ§in input_values ve labels ayrÄ± ayrÄ± pad edilir.
    """
    def __init__(self, processor):
        self.processor = processor

    def __call__(self, features):
        # input_values iÃ§in padding
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        batch = self.processor.feature_extractor.pad(
            input_features,
            padding=True,
            return_tensors="pt",
        )
        
        # labels iÃ§in padding
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(
            label_features,
            padding=True,
            return_tensors="pt",
        )
        
        # Padding token'larÄ± -100'e Ã§evir (CTC loss iÃ§in)
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch["attention_mask"].ne(1), 
            -100
        )
        batch["labels"] = labels
        return batch

def build_augment_pipeline(sampling_rate: int):
    """
    KonuÅŸma bozukluÄŸu iÃ§in optimize edilmiÅŸ augmentation pipeline.
    Hafif augmentation kullanÄ±r (aÅŸÄ±rÄ± distortion'dan kaÃ§Ä±nÄ±r).
    """
    if not AUDIOMENTATIONS_AVAILABLE:
        return None
    
    return A.Compose([
        # Hafif gÃ¼rÃ¼ltÃ¼ ekleme (konuÅŸma bozukluÄŸu iÃ§in dÃ¼ÅŸÃ¼k seviye)
        A.AddGaussianNoise(min_amplitude=0.0005, max_amplitude=0.005, p=0.3),
        # Zaman esnetme (konuÅŸma hÄ±zÄ± varyasyonu)
        A.TimeStretch(min_rate=0.9, max_rate=1.1, p=0.3, leave_length_unchanged=False),
        # Pitch deÄŸiÅŸimi (hafif, konuÅŸma bozukluÄŸu iÃ§in)
        A.PitchShift(min_semitones=-2, max_semitones=2, p=0.3),
        # Zaman maskesi (kÃ¼Ã§Ã¼k bÃ¶lÃ¼mler)
        A.TimeMask(min_band_part=0.03, max_band_part=0.1, p=0.2),
    ], p=0.6)  # %60 ihtimalle augmentation uygula

def _standalone_preprocess_function(examples, processor, augmenter=None):
    """
    Standalone data preprocessing function for multiprocessing.
    Wav2Vec2 iÃ§in Ã¶zellik Ã§Ä±karÄ±mÄ± ve tokenization yapar.
    Augmentation desteÄŸi eklenmiÅŸtir.
    """
    # Ses dosyalarÄ±nÄ± yÃ¼kle
    audio_arrays = []
    valid_transcripts = []
    
    # Transcript sÃ¼tununu belirle
    transcript_key = "transcript" if "transcript" in examples else "transcription"
    transcripts = examples.get(transcript_key, [""] * len(examples["file_path"]))
    
    for i, path_dict in enumerate(examples["file_path"]):
        try:
            audio, sr = librosa.load(path_dict['path'], sr=config.ORNEKLEME_ORANI)
            
            # Minimum uzunluk kontrolÃ¼ (en az 0.1 saniye)
            if len(audio) < config.ORNEKLEME_ORANI * 0.1:
                continue
            
            # Augmentation uygula (eÄŸer varsa)
            if augmenter is not None:
                try:
                    audio = augmenter(samples=audio, sample_rate=sr)
                except Exception as e:
                    # Augmentation hatasÄ± durumunda orijinal sesi kullan
                    pass
            
            # Transcript kontrolÃ¼
            transcript = str(transcripts[i]).strip() if i < len(transcripts) else ""
            if transcript:
                audio_arrays.append(audio)
                valid_transcripts.append(transcript)
        except Exception as e:
            # Hata durumunda sessizce atla
            continue
    
    if len(audio_arrays) == 0:
        # BoÅŸ batch iÃ§in dummy deÄŸerler dÃ¶ndÃ¼r
        return {
            "input_values": np.array([0.0]),
            "labels": [processor.tokenizer.pad_token_id]
        }
    
    # Processor ile Ã¶zellik Ã§Ä±karÄ±mÄ± (her Ã¶rnek iÃ§in ayrÄ± ayrÄ±)
    input_values_list = []
    label_ids_list = []
    valid_indices = []
    
    for i, (audio, transcript) in enumerate(zip(audio_arrays, valid_transcripts)):
        try:
            # Her Ã¶rnek iÃ§in ayrÄ± ayrÄ± iÅŸle
            inputs = processor(
                audio, 
                sampling_rate=config.ORNEKLEME_ORANI, 
                return_tensors="pt", 
                padding=False  # Padding yapma, collator yapacak
            )
            
            # Input values'Ä± list'e Ã§evir
            input_vals = inputs.input_values[0]
            if isinstance(input_vals, torch.Tensor):
                input_vals = input_vals.tolist()
            input_values_list.append(input_vals)
            
            # Transcript'i tokenize et
            # tokenizer() dict dÃ¶ndÃ¼rÃ¼r, input_ids'i al
            tokenized = processor.tokenizer(transcript)
            label_ids = tokenized.input_ids
            
            # Format kontrolÃ¼ ve dÃ¼zeltme
            if isinstance(label_ids, torch.Tensor):
                label_ids = label_ids.tolist()
            
            # EÄŸer nested list ise ([[1,2,3]] gibi) flatten et
            if label_ids and isinstance(label_ids[0], list):
                label_ids = label_ids[0]
            
            # BoÅŸ label kontrolÃ¼
            if not label_ids or len(label_ids) == 0:
                print(f"âš ï¸  BoÅŸ label: '{transcript}' -> atlanÄ±yor")
                continue
            
            # Label'larÄ±n geÃ§erli token ID'leri iÃ§erdiÄŸini kontrol et
            if any(not isinstance(tid, int) or tid < 0 for tid in label_ids):
                print(f"âš ï¸  GeÃ§ersiz label IDs: {label_ids[:5]}... -> atlanÄ±yor")
                continue
                
            label_ids_list.append(label_ids)
            valid_indices.append(i)
            
        except Exception as e:
            print(f"âš ï¸  Ã–rnek {i} iÅŸlenirken hata: {e}")
            continue
    
    # EÄŸer hiÃ§ geÃ§erli Ã¶rnek yoksa
    if len(input_values_list) == 0:
        return {
            "input_values": [[0.0]],
            "labels": [[processor.tokenizer.pad_token_id]]
        }
    
    # SonuÃ§larÄ± dict olarak dÃ¶ndÃ¼r (collator iÃ§in)
    result = {
        "input_values": input_values_list,
        "labels": label_ids_list
    }
    
    return result

class PersonalizedTrainer:
    def __init__(self, user_id, base_model_path=None):
        self.user_id = user_id
        self.base_model_path = base_model_path or config.MODEL_NAME
        self.user_data_path = Path(config.BASE_PATH) / self.user_id
        self.output_dir = Path("data/models/personalized_models") / self.user_id
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processor = None
        self.model = None
        self.wer_metric = evaluate.load("wer")
        self.cer_metric = evaluate.load("cer")
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.checkpoint_dir = self.output_dir / "checkpoints"

    def run(self):
        # Linux sunucu iÃ§in logging baÅŸlat
        log_file = setup_logging(self.user_id)
        logger = logging.getLogger(__name__)
        
        print(f"ğŸ¯ {self.user_id} iÃ§in kiÅŸiselleÅŸtirme sÃ¼reci baÅŸlÄ±yor.")
        print("="*50)
        logger.info(f"Training started for user: {self.user_id}")
        
        # Sistem bilgileri
        import platform
        logger.info(f"Platform: {platform.system()} {platform.release()}")
        logger.info(f"Python: {sys.version}")
        if torch.cuda.is_available():
            logger.info(f"CUDA Version: {torch.version.cuda}")
            logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        
        if not self.user_data_path.exists() or not (self.user_data_path / "metadata_words.csv").exists():
            error_msg = f"âŒ Hata: {self.user_data_path} iÃ§in veri bulunamadÄ±."
            print(error_msg)
            logger.error(error_msg)
            return

        self.load_model_and_processor()
        train_dataset = self.prepare_dataset(split='train')
        eval_dataset = self.prepare_dataset(split='eval')
        self.train_model(train_dataset, eval_dataset)
        
        logger.info(f"Training completed for user: {self.user_id}")

    def load_model_and_processor(self):
        print(f"ğŸ“¥ Temel model yÃ¼kleniyor: {self.base_model_path}")
        self.processor = Wav2Vec2Processor.from_pretrained(self.base_model_path)
        self.model = Wav2Vec2ForCTC.from_pretrained(self.base_model_path)
        
        # Gradient checkpointing (opsiyonel, VRAM tasarrufu iÃ§in)
        if config.GRADIENT_CHECKPOINTING and hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
            print("   âœ… Gradient checkpointing aktif (VRAM tasarrufu)")
        
        self.model.to(self.device)
        
        # KonuÅŸma bozukluÄŸu iÃ§in optimize edilmiÅŸ LoRA konfigÃ¼rasyonu
        # Daha fazla modÃ¼l ve daha yÃ¼ksek rank kullanÄ±yoruz
        # Not: PEFT otomatik olarak Wav2Vec2ForCTC model tipini algÄ±lar, task_type gerekmez
        peft_config = LoraConfig(
            r=config.ADAPTER_REDUCTION_FACTOR,
            lora_alpha=config.ADAPTER_REDUCTION_FACTOR * 2,
            target_modules=["q_proj", "v_proj", "k_proj", "out_proj"],  # Daha fazla modÃ¼l
            lora_dropout=0.05,  # Daha dÃ¼ÅŸÃ¼k dropout (overfitting riski dÃ¼ÅŸÃ¼k)
            bias="none",
            # task_type parametresi kaldÄ±rÄ±ldÄ± - PEFT otomatik algÄ±lar
        )
        self.model = get_peft_model(self.model, peft_config)
        
        # Trainable parametreleri gÃ¶ster
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"âœ… Model PEFT/LoRA ile sarmalandÄ±. Cihaz: {self.device}")
        print(f"   EÄŸitilebilir parametreler: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)")

    def prepare_dataset(self, split='train'):
        """
        Veri setini hazÄ±rlar ve yÃ¼kler.
        
        Args:
            split: 'train' veya 'eval'
        """
        print(f"ğŸ“Š {split.upper()} veri seti hazÄ±rlanÄ±yor: {self.user_data_path}")
        
        # Split'e gÃ¶re dosya seÃ§
        if split == 'train':
            train_csv = self.user_data_path / "train.csv"
            if train_csv.exists():
                print(f"   âœ… train.csv bulundu, kullanÄ±lÄ±yor.")
                df = pd.read_csv(train_csv, encoding='utf-8')
            else:
                # metadata_words.csv'den oluÅŸtur
                metadata_path = self.user_data_path / "metadata_words.csv"
                if not metadata_path.exists():
                    raise FileNotFoundError(
                        f"âŒ Hata: Ne train.csv ne de metadata_words.csv bulunamadÄ±!\n"
                        f"   LÃ¼tfen Ã¶nce 'python prepare_training_data.py {self.user_id}' Ã§alÄ±ÅŸtÄ±rÄ±n."
                    )
                
                print(f"   âš ï¸  train.csv bulunamadÄ±, metadata_words.csv kullanÄ±lÄ±yor.")
                df = pd.read_csv(metadata_path, encoding='utf-8')
                df = df[['file_path', 'transcription']].copy()
                df.rename(columns={'transcription': 'transcript'}, inplace=True)
        else:  # eval
            eval_csv = self.user_data_path / "eval.csv"
            if eval_csv.exists():
                print(f"   âœ… eval.csv bulundu, kullanÄ±lÄ±yor.")
                df = pd.read_csv(eval_csv, encoding='utf-8')
            else:
                # metadata_words.csv'den oluÅŸtur (validation iÃ§in)
                metadata_path = self.user_data_path / "metadata_words.csv"
                if not metadata_path.exists():
                    print(f"   âš ï¸  eval.csv bulunamadÄ±, validation seti oluÅŸturulamÄ±yor.")
                    return None
                
                print(f"   âš ï¸  eval.csv bulunamadÄ±, metadata_words.csv'nin %20'si validation iÃ§in kullanÄ±lÄ±yor.")
                df = pd.read_csv(metadata_path, encoding='utf-8')
                df = df[['file_path', 'transcription']].copy()
                df.rename(columns={'transcription': 'transcript'}, inplace=True)
                # Son %20'yi validation iÃ§in al
                df = df.tail(int(len(df) * 0.2))

        # Dosya yollarÄ±nÄ± dÃ¼zelt
        words_dir = self.user_data_path / "words"
        def fix_file_path(path):
            filename = os.path.basename(str(path))
            return str(words_dir / filename)
        
        df["file_path"] = df["file_path"].apply(fix_file_path)
        
        # Var olmayan dosyalarÄ± filtrele
        original_size = len(df)
        df = df[df["file_path"].apply(os.path.exists)]
        if len(df) < original_size:
            print(f"   âš ï¸  {original_size - len(df)} adet bulunamayan ses dosyasÄ± atlandÄ±.")
        
        if len(df) == 0:
            if split == 'eval':
                return None
            raise ValueError(f"âŒ Hata: HiÃ§ geÃ§erli ses dosyasÄ± bulunamadÄ±!")
        
        # BoÅŸ transkriptleri filtrele
        df = df[df['transcript'].notna() & (df['transcript'].str.strip() != '')]
        
        dataset = Dataset.from_pandas(df)
        dataset = dataset.cast_column("file_path", Audio(sampling_rate=config.ORNEKLEME_ORANI, decode=False))
        
        print(f"   ğŸ“ˆ {split.upper()} veri seti boyutu: {len(dataset)} kayÄ±t")
        return dataset

    def evaluate_model(self, eval_dataloader, accelerator):
        """Validation setinde modeli deÄŸerlendirir."""
        self.model.eval()
        total_loss = 0.0
        all_predictions = []
        all_references = []
        num_batches = 0
        
        with torch.no_grad():
            for batch in eval_dataloader:
                outputs = self.model(**batch)
                loss = outputs.loss
                total_loss += loss.item()
                num_batches += 1
                
                # WER/CER hesaplama iÃ§in tahminler
                logits = outputs.logits
                predicted_ids = torch.argmax(logits, dim=-1)
                predictions = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)
                
                # Referans metinleri
                label_ids = batch["labels"]
                label_ids[label_ids == -100] = self.processor.tokenizer.pad_token_id
                references = self.processor.batch_decode(label_ids, skip_special_tokens=True)
                
                all_predictions.extend(predictions)
                all_references.extend(references)
        
        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
        
        # WER ve CER hesapla
        wer = self.wer_metric.compute(predictions=all_predictions, references=all_references)
        cer = self.cer_metric.compute(predictions=all_predictions, references=all_references)
        
        self.model.train()
        return avg_loss, wer, cer

    def train_model(self, train_dataset, eval_dataset=None):
        """Model eÄŸitimini baÅŸlatÄ±r."""
        print("ğŸš€ KiÅŸiselleÅŸtirilmiÅŸ model eÄŸitimi baÅŸlÄ±yor...")
        print(f"   Epoch sayÄ±sÄ±: {config.NUM_FINETUNE_EPOCHS}")
        print(f"   Batch size: {config.FINETUNE_BATCH_SIZE}")
        print(f"   Learning rate: {config.FINETUNE_LEARNING_RATE}")
        print(f"   Gradient accumulation: {config.GRADIENT_ACCUMULATION_STEPS}")
        print(f"   Augmentation: {'Aktif' if config.USE_AUGMENTATION and AUDIOMENTATIONS_AVAILABLE else 'Pasif'}")
        print(f"   Validation: {'Aktif' if eval_dataset is not None else 'Pasif'}")

        # Augmentation pipeline oluÅŸtur
        augmenter = None
        if config.USE_AUGMENTATION and AUDIOMENTATIONS_AVAILABLE:
            augmenter = build_augment_pipeline(config.ORNEKLEME_ORANI)

        # Veri Ã¶n iÅŸleme (sistem kaynaklarÄ±na gÃ¶re optimize)
        num_proc = min(config.DATA_PREPROCESSING_NUM_PROC, os.cpu_count() or 1)
        print(f"\nâš™ï¸  Veri Ã¶n iÅŸleme {num_proc} CPU Ã§ekirdeÄŸi ile paralelleÅŸtiriliyor...")
        
        try:
            # Training set preprocessing (with augmentation)
            processed_train_dataset = train_dataset.map(
                _standalone_preprocess_function,
                fn_kwargs={"processor": self.processor, "augmenter": augmenter},
                remove_columns=train_dataset.column_names,
                batched=True,
                batch_size=config.FINETUNE_BATCH_SIZE,
                num_proc=num_proc
            )
            
            # BoÅŸ Ã¶rnekleri filtrele
            processed_train_dataset = processed_train_dataset.filter(
                lambda x: len(x.get("input_values", [])) > 0 and len(x.get("labels", [])) > 0
            )
            
            if len(processed_train_dataset) == 0:
                raise ValueError("âŒ Hata: Ã–n iÅŸleme sonrasÄ± hiÃ§ geÃ§erli Ã¶rnek kalmadÄ±!")
            
            print(f"   âœ… Training set Ã¶n iÅŸleme tamamlandÄ±. {len(processed_train_dataset)} geÃ§erli Ã¶rnek.")
            
            # Validation set preprocessing (no augmentation)
            processed_eval_dataset = None
            eval_dataloader = None
            if eval_dataset is not None:
                processed_eval_dataset = eval_dataset.map(
                    _standalone_preprocess_function,
                    fn_kwargs={"processor": self.processor, "augmenter": None},
                    remove_columns=eval_dataset.column_names,
                    batched=True,
                    batch_size=config.FINETUNE_BATCH_SIZE,
                    num_proc=num_proc
                )
                
                processed_eval_dataset = processed_eval_dataset.filter(
                    lambda x: len(x.get("input_values", [])) > 0 and len(x.get("labels", [])) > 0
                )
                
                if len(processed_eval_dataset) > 0:
                    print(f"   âœ… Validation set Ã¶n iÅŸleme tamamlandÄ±. {len(processed_eval_dataset)} geÃ§erli Ã¶rnek.")
                else:
                    print(f"   âš ï¸  Validation seti boÅŸ, validation atlanacak.")
                    processed_eval_dataset = None
            
        except Exception as e:
            print(f"âŒ Veri Ã¶n iÅŸleme hatasÄ±: {e}")
            import traceback
            traceback.print_exc()
            return

        # Data collator ve dataloader
        data_collator = DataCollatorCTCWithPadding(processor=self.processor)

        # RTX A5000 iÃ§in optimize edilmiÅŸ DataLoader ayarlarÄ±
        train_dataloader = DataLoader(
            processed_train_dataset,
            batch_size=config.FINETUNE_BATCH_SIZE,
            collate_fn=data_collator,
            shuffle=True,
            num_workers=config.DATALOADER_NUM_WORKERS,
            pin_memory=config.DATALOADER_PIN_MEMORY,
            prefetch_factor=config.DATALOADER_PREFETCH_FACTOR if config.DATALOADER_NUM_WORKERS > 0 else None,
            persistent_workers=True if config.DATALOADER_NUM_WORKERS > 0 else False
        )
        
        if processed_eval_dataset is not None:
            eval_dataloader = DataLoader(
                processed_eval_dataset,
                batch_size=config.FINETUNE_BATCH_SIZE,
                collate_fn=data_collator,
                shuffle=False,
                num_workers=config.DATALOADER_NUM_WORKERS,
                pin_memory=config.DATALOADER_PIN_MEMORY,
                prefetch_factor=config.DATALOADER_PREFETCH_FACTOR if config.DATALOADER_NUM_WORKERS > 0 else None,
                persistent_workers=True if config.DATALOADER_NUM_WORKERS > 0 else False
            )

        # Optimizer with warmup
        optimizer = AdamW(
            self.model.parameters(), 
            lr=config.FINETUNE_LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY
        )
        
        # Learning rate scheduler (linear warmup)
        num_training_steps = config.NUM_FINETUNE_EPOCHS * len(train_dataloader)
        warmup_steps = config.WARMUP_STEPS
        from transformers import get_linear_schedule_with_warmup
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=num_training_steps
        )
        
        # Accelerator (RTX A5000 iÃ§in optimize edilmiÅŸ ayarlar)
        mixed_precision_mode = config.MIXED_PRECISION if torch.cuda.is_available() else "no"
        if mixed_precision_mode == "fp16" and not torch.cuda.is_available():
            print("âš ï¸  FP16 seÃ§ildi ancak CUDA yok, mixed precision kapatÄ±lÄ±yor.")
            mixed_precision_mode = "no"
        
        accelerator = Accelerator(
            mixed_precision=mixed_precision_mode,
            gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS
        )
        
        if torch.cuda.is_available():
            print(f"âœ… GPU kullanÄ±lÄ±yor: {torch.cuda.get_device_name(0)}")
            print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
            print(f"   Mixed Precision: {mixed_precision_mode}")
            print(f"   Batch Size: {config.FINETUNE_BATCH_SIZE}")
            print(f"   Gradient Accumulation: {config.GRADIENT_ACCUMULATION_STEPS}")
            print(f"   Effective Batch Size: {config.FINETUNE_BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}")
        else:
            print("âš ï¸  CUDA yok, CPU kullanÄ±lÄ±yor.")
        
        self.model, optimizer, train_dataloader, scheduler = accelerator.prepare(
            self.model, optimizer, train_dataloader, scheduler
        )
        
        if eval_dataloader is not None:
            eval_dataloader = accelerator.prepare(eval_dataloader)

        # EÄŸitim dÃ¶ngÃ¼sÃ¼
        num_epochs = config.NUM_FINETUNE_EPOCHS
        progress_bar = tqdm(range(num_training_steps), desc="EÄŸitim")
        
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            global_step = 0
            for epoch in range(num_epochs):
                epoch_loss = 0.0
                num_batches = 0
                
                self.model.train()
                for step, batch in enumerate(train_dataloader):
                    with accelerator.accumulate(self.model):
                        outputs = self.model(**batch)
                        loss = outputs.loss
                        
                        # Loss kontrolÃ¼ (negatif veya invalid loss iÃ§in)
                        if torch.isnan(loss) or torch.isinf(loss):
                            print(f"âš ï¸  Epoch {epoch+1}, Step {step}: GeÃ§ersiz loss ({loss.item()}), batch atlanÄ±yor.")
                            continue
                        
                        if loss.item() < 0:
                            print(f"âš ï¸  Epoch {epoch+1}, Step {step}: Negatif loss ({loss.item()}), batch atlanÄ±yor.")
                            # Debug iÃ§in batch bilgilerini yazdÄ±r (sadece ilk batch'te)
                            if step == 0 and epoch == 0:
                                print(f"   Debug - Batch keys: {list(batch.keys())}")
                                print(f"   Debug - Input shape: {batch['input_values'].shape}")
                                print(f"   Debug - Labels shape: {batch['labels'].shape}")
                                print(f"   Debug - Labels min/max: {batch['labels'].min().item()}/{batch['labels'].max().item()}")
                                print(f"   Debug - Labels sample (first 10): {batch['labels'][0][:10].tolist()}")
                                # Ä°lk birkaÃ§ label'Ä± decode et
                                try:
                                    sample_labels = batch['labels'][0].clone()
                                    sample_labels[sample_labels == -100] = self.processor.tokenizer.pad_token_id
                                    decoded = self.processor.tokenizer.decode(sample_labels[:20], skip_special_tokens=False)
                                    print(f"   Debug - Decoded sample: {decoded}")
                                except Exception as e:
                                    print(f"   Debug - Decode hatasÄ±: {e}")
                            continue
                        
                        accelerator.backward(loss)
                        
                        # Gradient clipping (stabilite iÃ§in)
                        if accelerator.sync_gradients:
                            accelerator.clip_grad_norm_(self.model.parameters(), max_norm=config.MAX_GRAD_NORM)
                        
                        optimizer.step()
                        scheduler.step()
                        optimizer.zero_grad()
                    
                    epoch_loss += loss.item()
                    num_batches += 1
                    global_step += 1
                    
                    progress_bar.update(1)
                    avg_loss = epoch_loss / num_batches
                    current_lr = optimizer.param_groups[0]['lr']
                    progress_bar.set_description(
                        f"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | LR: {current_lr:.2e}"
                    )
                    
                    # Validation (belirli adÄ±mlarda)
                    if eval_dataloader is not None and global_step % config.FINETUNE_EVAL_STEPS == 0:
                        val_loss, wer, cer = self.evaluate_model(eval_dataloader, accelerator)
                        print(f"\n   ğŸ“Š Validation (Step {global_step}):")
                        print(f"      Loss: {val_loss:.4f} | WER: {wer:.4f} ({wer*100:.2f}%) | CER: {cer:.4f} ({cer*100:.2f}%)")
                        
                        # Early stopping kontrolÃ¼
                        if val_loss < self.best_val_loss:
                            self.best_val_loss = val_loss
                            self.patience_counter = 0
                            # Best model'i kaydet
                            unwrapped_model = accelerator.unwrap_model(self.model)
                            best_checkpoint = self.checkpoint_dir / "best_model"
                            save_model_and_processor(unwrapped_model, self.processor, str(best_checkpoint))
                            print(f"      âœ… Yeni en iyi model kaydedildi! (Loss: {val_loss:.4f})")
                        else:
                            self.patience_counter += 1
                            if self.patience_counter >= config.EARLY_STOPPING_PATIENCE:
                                print(f"\n   â¹ï¸  Early stopping tetiklendi! (Patience: {config.EARLY_STOPPING_PATIENCE})")
                                print(f"      En iyi validation loss: {self.best_val_loss:.4f}")
                                break
                
                avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0.0
                print(f"\n   Epoch {epoch+1}/{num_epochs} tamamlandÄ±. Ortalama Loss: {avg_epoch_loss:.4f}")
                
                # Epoch sonunda validation
                if eval_dataloader is not None:
                    val_loss, wer, cer = self.evaluate_model(eval_dataloader, accelerator)
                    print(f"   ğŸ“Š Epoch sonu Validation:")
                    print(f"      Loss: {val_loss:.4f} | WER: {wer:.4f} ({wer*100:.2f}%) | CER: {cer:.4f} ({cer*100:.2f}%)")
                
                # Early stopping kontrolÃ¼
                if self.patience_counter >= config.EARLY_STOPPING_PATIENCE:
                    break

            print("\nâœ… Model ince ayarÄ± tamamlandÄ±!")
            
            # En iyi modeli yÃ¼kle ve kaydet
            if (self.checkpoint_dir / "best_model").exists():
                print(f"   ğŸ“¥ En iyi model yÃ¼kleniyor...")
                from peft import PeftModel
                base_model = Wav2Vec2ForCTC.from_pretrained(self.base_model_path)
                best_model = PeftModel.from_pretrained(base_model, str(self.checkpoint_dir / "best_model"))
                unwrapped_model = accelerator.unwrap_model(best_model)
            else:
                unwrapped_model = accelerator.unwrap_model(self.model)
            
            self.output_dir.mkdir(parents=True, exist_ok=True)
            save_model_and_processor(unwrapped_model, self.processor, str(self.output_dir))

            print(f"ğŸ’¾ KiÅŸiselleÅŸtirilmiÅŸ model kaydedildi: {self.output_dir}")
            print(f"   Toplam eÄŸitim adÄ±mÄ±: {global_step}")
            if eval_dataloader is not None:
                final_val_loss, final_wer, final_cer = self.evaluate_model(eval_dataloader, accelerator)
                print(f"   Final Validation - Loss: {final_val_loss:.4f} | WER: {final_wer:.4f} ({final_wer*100:.2f}%) | CER: {final_cer:.4f} ({final_cer*100:.2f}%)")
            
        except Exception as e:
            print(f"\nâŒ EÄŸitim sÄ±rasÄ±nda hata oluÅŸtu: {e}")
            import traceback
            traceback.print_exc()
            raise

def main():
    parser = argparse.ArgumentParser(description="KullanÄ±cÄ±ya Ã¶zel ASR modelini eÄŸitir.")
    parser.add_argument("user_id", type=str, help="Verisi kullanÄ±lacak ve modeli kiÅŸiselleÅŸtirilecek kullanÄ±cÄ±nÄ±n kimliÄŸi.")
    parser.add_argument("--base_model", type=str, help="Ä°nce ayar iÃ§in kullanÄ±lacak temel modelin yolu. VarsayÄ±lan: config.py'deki model.", default=None)
    
    args = parser.parse_args()
    
    trainer = PersonalizedTrainer(user_id=args.user_id, base_model_path=args.base_model)
    trainer.run()

if __name__ == "__main__":
    import platform
    import sys
    
    # Linux sunucu iÃ§in multiprocessing optimizasyonu
    if platform.system() == "Linux":
        import multiprocessing as mp
        try:
            # Linux'ta fork daha hÄ±zlÄ± ve verimli
            mp.set_start_method(config.MULTIPROCESSING_START_METHOD, force=True)
            print(f"âœ… Linux sunucu: Multiprocessing start method = {config.MULTIPROCESSING_START_METHOD}")
        except RuntimeError:
            # Zaten ayarlanmÄ±ÅŸsa devam et
            pass
    else:
        # Windows/Mac iÃ§in spawn
        if torch.cuda.is_available():
            try:
                import multiprocessing as mp
                mp.set_start_method("spawn", force=True)
            except RuntimeError:
                pass
    
    # CUDA device seÃ§imi (Linux sunucuda birden fazla GPU varsa)
    if torch.cuda.is_available() and config.CUDA_VISIBLE_DEVICES is not None:
        os.environ["CUDA_VISIBLE_DEVICES"] = str(config.CUDA_VISIBLE_DEVICES)
        print(f"âœ… CUDA_VISIBLE_DEVICES = {config.CUDA_VISIBLE_DEVICES}")

    main()