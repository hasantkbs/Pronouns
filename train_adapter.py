import os
import argparse
import torch
import pandas as pd
from pathlib import Path
import dataclasses
from typing import List, Dict, Union
from transformers import (
    WhisperForConditionalGeneration,
    WhisperProcessor,
    TrainingArguments,
    Trainer,
)
from peft import LoraConfig, get_peft_model
from torch.utils.data import DataLoader
from torch.optim import AdamW
from accelerate import Accelerator
from tqdm import tqdm
import config
import librosa
import soundfile as sf
from datasets import Dataset, Audio
from src.utils.utils import save_model


@dataclasses.dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # split inputs and labels since they have to be of different lengths and need
        # different padding methods
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # if bos token is appended in previous tokenization step,
        # cut bos token here as it's append later anyways
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch

class PersonalizedTrainer:
    """KullanÄ±cÄ±ya Ã¶zel model eÄŸitici."""
    
    def __init__(self, user_id, base_model_path=None):
        self.user_id = user_id
        self.base_model_path = base_model_path or "openai/whisper-large-v2" # Default Whisper large model
        self.user_data_path = Path(config.BASE_PATH) / self.user_id
        self.output_dir = Path("data/models/personalized_models") / self.user_id
        self.adapter_name = "user_adapter"
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processor = None
        self.model = None

    def run(self):
        """KiÅŸiselleÅŸtirme sÃ¼recini baÅŸlatÄ±r."""
        print(f"ğŸ¯ {self.user_id} iÃ§in kiÅŸiselleÅŸtirme sÃ¼reci baÅŸlÄ±yor.")
        print("="*50)
        
        if not self.user_data_path.exists() or not (self.user_data_path / "metadata_words.csv").exists():
            print(f"âŒ Hata: {self.user_data_path} iÃ§in veri bulunamadÄ±.")
            print(f"LÃ¼tfen Ã¶nce 'src/training/collect_user_data.py' scriptini Ã§alÄ±ÅŸtÄ±rÄ±n.")
            return

        self.load_model_and_processor()
        dataset = self.prepare_dataset()
        self.train_model(dataset)

    def load_model_and_processor(self):
        """Temel model ve iÅŸlemciyi yÃ¼kler."""
        print(f"ğŸ“¥ Temel model yÃ¼kleniyor: {self.base_model_path}")
        self.processor = WhisperProcessor.from_pretrained(self.base_model_path, language="tr", task="transcribe")

        # Performans Optimizasyonu: MÃ¼mkÃ¼nse Flash Attention 2'yi etkinleÅŸtir
        try:
            self.model = WhisperForConditionalGeneration.from_pretrained(self.base_model_path, attn_implementation="flash_attention_2")
            print("âš¡ï¸ Model, Flash Attention 2 optimizasyonu ile yÃ¼kleniyor.")
        except (ImportError, ValueError):
            print("âš ï¸ Flash Attention 2 kullanÄ±lamÄ±yor. Standart dikkat mekanizmasÄ± ile devam ediliyor.")
            self.model = WhisperForConditionalGeneration.from_pretrained(self.base_model_path)
        
        self.model.to(self.device)
        peft_config = LoraConfig(
            r=config.ADAPTER_REDUCTION_FACTOR,
            lora_alpha=config.ADAPTER_REDUCTION_FACTOR * 2, # A common heuristic
            target_modules=["q_proj", "v_proj", "k_proj"], # Common target modules for Whisper
            lora_dropout=0.1, # Example dropout
            bias="none",
        )
        self.model = get_peft_model(self.model, peft_config)
        print(f"âœ… Model PEFT/LoRA ile sarmalandÄ±. Cihaz: {self.device}")

    def prepare_dataset(self):
        """KullanÄ±cÄ±ya Ã¶zel veri setini hazÄ±rlar."""
        print(f"ğŸ“Š Veri seti hazÄ±rlanÄ±yor: {self.user_data_path}")
        metadata_path = self.user_data_path / "metadata_words.csv"
        df = pd.read_csv(metadata_path)

        def audio_loader(path):
            filename = os.path.basename(path)
            filepath = self.user_data_path / "words" / filename # Use self.user_data_path
            try:
                speech, sample_rate = librosa.load(filepath, sr=config.ORNEKLEME_ORANI)
                return speech
            except FileNotFoundError:
                print(f"âš ï¸  UyarÄ±: Ses dosyasÄ± bulunamadÄ±, atlanÄ±yor: {filepath}")
                return None
            except Exception as e:
                print(f"âŒ Hata yÃ¼klenirken: {filepath} - {e}")
                return None

        df["audio"] = df["file_path"].apply(audio_loader)
        # Remove rows where audio loading failed
        df = df.dropna(subset=["audio"])
        
        dataset = Dataset.from_pandas(df)
        print(f"ğŸ“ˆ Veri seti boyutu: {len(dataset)} kayÄ±t")
        return dataset

    def preprocess_function(self, examples):
        """Veri Ã¶n iÅŸleme fonksiyonu."""
        audio_arrays = [x for x in examples["audio"]]
        
        model_inputs = self.processor(audio_arrays, sampling_rate=config.ORNEKLEME_ORANI, return_tensors="pt", padding="max_length", truncation=True)
        
        labels = self.processor.tokenizer(text=examples["transcription"], padding=True, truncation=True).input_ids
        model_inputs["labels"] = labels

        return model_inputs

    def train_model(self, dataset):
        """Modeli, transformers.Trainer kullanmadan manuel bir PyTorch dÃ¶ngÃ¼sÃ¼ ile eÄŸitir."""
        print("ğŸš€ KiÅŸiselleÅŸtirilmiÅŸ model eÄŸitimi baÅŸlÄ±yor... (Manuel DÃ¶ngÃ¼)")

        # 1. Veri Setini HazÄ±rla
        # Not: num_proc > 1 kullanmak Windows'ta 'fork' metodu nedeniyle sorun yaratabilir. 
        # Sorun yaÅŸarsanÄ±z bu deÄŸeri 1'e dÃ¼ÅŸÃ¼rÃ¼n veya bu satÄ±rÄ± kaldÄ±rÄ±n.
        try:
            num_cpus = os.cpu_count()
        except NotImplementedError:
            num_cpus = 1
        print(f"âš™ï¸  Veri Ã¶n iÅŸleme {num_cpus} CPU Ã§ekirdeÄŸi ile paralelleÅŸtiriliyor...")
        
        processed_dataset = dataset.map(
            self.preprocess_function,
            remove_columns=dataset.column_names,
            batched=True,
            batch_size=config.FINETUNE_BATCH_SIZE,
            num_proc=num_cpus
        )

        data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=self.processor)

        dataloader = DataLoader(
            processed_dataset,
            batch_size=config.FINETUNE_BATCH_SIZE,
            collate_fn=data_collator
        )

        # 2. Optimizasyon ve HÄ±zlandÄ±rÄ±cÄ± (Accelerator) AyarlarÄ±
        optimizer = AdamW(self.model.parameters(), lr=config.FINETUNE_LEARNING_RATE)
        
        accelerator = Accelerator(
            mixed_precision="fp16" if torch.cuda.is_available() else "no",
            gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS
        )
        
        self.model, optimizer, dataloader = accelerator.prepare(
            self.model, optimizer, dataloader
        )

        num_epochs = config.NUM_FINETUNE_EPOCHS
        num_training_steps = num_epochs * len(dataloader)
        progress_bar = tqdm(range(num_training_steps))

        # 3. EÄŸitim DÃ¶ngÃ¼sÃ¼
        self.model.train()
        for epoch in range(num_epochs):
            for step, batch in enumerate(dataloader):
                with accelerator.accumulate(self.model):
                    # Forward pass
                    outputs = self.model(**batch)
                    loss = outputs.loss
                    
                    # Backward pass
                    accelerator.backward(loss)
                    
                    optimizer.step()
                    optimizer.zero_grad()
                
                progress_bar.update(1)
                progress_bar.set_description(f"Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.4f}")

        # 4. Modeli Kaydet
        print("\nâœ… Model ince ayarÄ± tamamlandÄ±!")
        
        # Modeli unwrapping iÅŸlemi ve kaydetme
        unwrapped_model = accelerator.unwrap_model(self.model)
        save_model(unwrapped_model, self.processor, str(self.output_dir))

        print(f"ğŸ’¾ KiÅŸiselleÅŸtirilmiÅŸ model kaydedildi: {self.output_dir}")
        print("\nKullanÄ±m iÃ§in app.py veya config.py dosyasÄ±nÄ± bu yeni model yolunu kullanacak ÅŸekilde gÃ¼ncelleyebilirsiniz.")

def main():
    parser = argparse.ArgumentParser(description="KullanÄ±cÄ±ya Ã¶zel ASR modelini eÄŸitir.")
    parser.add_argument("user_id", type=str, help="Verisi kullanÄ±lacak ve modeli kiÅŸiselleÅŸtirilecek kullanÄ±cÄ±nÄ±n kimliÄŸi.")
    parser.add_argument("--base_model", type=str, help="Ä°nce ayar iÃ§in kullanÄ±lacak temel modelin yolu. VarsayÄ±lan: config.py'deki model.", default=None)
    
    args = parser.parse_args()
    
    trainer = PersonalizedTrainer(user_id=args.user_id, base_model_path=args.base_model)
    trainer.run()

if __name__ == "__main__":
    main()