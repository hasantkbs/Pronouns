# train_adapter.py

import os
import argparse
import torch
import pandas as pd
from pathlib import Path
from transformers import (
    Wav2Vec2ForCTC,
    Wav2Vec2Processor,
)
from peft import LoraConfig, get_peft_model
from torch.utils.data import DataLoader
from torch.optim import AdamW
from accelerate import Accelerator
from tqdm import tqdm
from datasets import Dataset, Audio
from src.utils.utils import save_model_and_processor
import config
import librosa

class DataCollatorCTCWithPadding:
    """
    CTC tabanlÄ± ASR iÃ§in data collator.
    Wav2Vec2 iÃ§in input_values ve labels ayrÄ± ayrÄ± pad edilir.
    """
    def __init__(self, processor):
        self.processor = processor

    def __call__(self, features):
        # input_values iÃ§in padding
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        batch = self.processor.feature_extractor.pad(
            input_features,
            padding=True,
            return_tensors="pt",
        )
        
        # labels iÃ§in padding
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(
            label_features,
            padding=True,
            return_tensors="pt",
        )
        
        # Padding token'larÄ± -100'e Ã§evir (CTC loss iÃ§in)
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch["attention_mask"].ne(1), 
            -100
        )
        batch["labels"] = labels
        return batch

def _standalone_preprocess_function(examples, processor):
    """
    Standalone data preprocessing function for multiprocessing.
    Wav2Vec2 iÃ§in Ã¶zellik Ã§Ä±karÄ±mÄ± ve tokenization yapar.
    """
    import numpy as np
    
    # Ses dosyalarÄ±nÄ± yÃ¼kle
    audio_arrays = []
    valid_transcripts = []
    
    # Transcript sÃ¼tununu belirle
    transcript_key = "transcript" if "transcript" in examples else "transcription"
    transcripts = examples.get(transcript_key, [""] * len(examples["file_path"]))
    
    for i, path_dict in enumerate(examples["file_path"]):
        try:
            audio, sr = librosa.load(path_dict['path'], sr=config.ORNEKLEME_ORANI)
            if len(audio) > 100:  # En az 100 sample (Ã§ok kÄ±sa kayÄ±tlarÄ± filtrele)
                # Transcript kontrolÃ¼
                transcript = str(transcripts[i]).strip() if i < len(transcripts) else ""
                if transcript:
                    audio_arrays.append(audio)
                    valid_transcripts.append(transcript)
        except Exception as e:
            # Hata durumunda sessizce atla (loglama Ã§ok fazla olabilir)
            continue
    
    if len(audio_arrays) == 0:
        # BoÅŸ batch iÃ§in dummy deÄŸerler dÃ¶ndÃ¼r
        return {
            "input_values": np.array([0.0]),
            "labels": [processor.tokenizer.pad_token_id]
        }
    
    # Processor ile Ã¶zellik Ã§Ä±karÄ±mÄ±
    inputs = processor(
        audio_arrays, 
        sampling_rate=config.ORNEKLEME_ORANI, 
        return_tensors="pt", 
        padding=True
    )
    
    # Transkriptleri tokenize et
    labels = processor.tokenizer(
        valid_transcripts, 
        return_tensors="pt", 
        padding=True
    ).input_ids
    
    # SonuÃ§larÄ± dict olarak dÃ¶ndÃ¼r (collator iÃ§in)
    result = {
        "input_values": inputs.input_values,
        "labels": labels
    }
    
    return result

class PersonalizedTrainer:
    def __init__(self, user_id, base_model_path=None):
        self.user_id = user_id
        self.base_model_path = base_model_path or config.MODEL_NAME
        self.user_data_path = Path(config.BASE_PATH) / self.user_id
        self.output_dir = Path("data/models/personalized_models") / self.user_id
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processor = None
        self.model = None

    def run(self):
        print(f"ğŸ¯ {self.user_id} iÃ§in kiÅŸiselleÅŸtirme sÃ¼reci baÅŸlÄ±yor.")
        print("="*50)
        
        if not self.user_data_path.exists() or not (self.user_data_path / "metadata_words.csv").exists():
            print(f"âŒ Hata: {self.user_data_path} iÃ§in veri bulunamadÄ±.")
            return

        self.load_model_and_processor()
        dataset = self.prepare_dataset()
        self.train_model(dataset)

    def load_model_and_processor(self):
        print(f"ğŸ“¥ Temel model yÃ¼kleniyor: {self.base_model_path}")
        self.processor = Wav2Vec2Processor.from_pretrained(self.base_model_path)
        self.model = Wav2Vec2ForCTC.from_pretrained(self.base_model_path)
        
        self.model.to(self.device)
        peft_config = LoraConfig(
            r=config.ADAPTER_REDUCTION_FACTOR,
            lora_alpha=config.ADAPTER_REDUCTION_FACTOR * 2,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.1,
            bias="none",
        )
        self.model = get_peft_model(self.model, peft_config)
        print(f"âœ… Model PEFT/LoRA ile sarmalandÄ±. Cihaz: {self.device}")

    def prepare_dataset(self):
        """Veri setini hazÄ±rlar ve yÃ¼kler."""
        print(f"ğŸ“Š Veri seti hazÄ±rlanÄ±yor: {self.user_data_path}")
        
        # Ã–nce train.csv ve eval.csv dosyalarÄ±nÄ± kontrol et
        train_csv = self.user_data_path / "train.csv"
        eval_csv = self.user_data_path / "eval.csv"
        
        if train_csv.exists():
            print(f"   âœ… train.csv bulundu, kullanÄ±lÄ±yor.")
            df = pd.read_csv(train_csv, encoding='utf-8')
        else:
            # metadata_words.csv'den oluÅŸtur
            metadata_path = self.user_data_path / "metadata_words.csv"
            if not metadata_path.exists():
                raise FileNotFoundError(
                    f"âŒ Hata: Ne train.csv ne de metadata_words.csv bulunamadÄ±!\n"
                    f"   LÃ¼tfen Ã¶nce 'python prepare_training_data.py {self.user_id}' Ã§alÄ±ÅŸtÄ±rÄ±n."
                )
            
            print(f"   âš ï¸  train.csv bulunamadÄ±, metadata_words.csv kullanÄ±lÄ±yor.")
            df = pd.read_csv(metadata_path, encoding='utf-8')
            df = df[['file_path', 'transcription']].copy()
            df.rename(columns={'transcription': 'transcript'}, inplace=True)

        # Dosya yollarÄ±nÄ± dÃ¼zelt
        words_dir = self.user_data_path / "words"
        def fix_file_path(path):
            filename = os.path.basename(str(path))
            return str(words_dir / filename)
        
        df["file_path"] = df["file_path"].apply(fix_file_path)
        
        # Var olmayan dosyalarÄ± filtrele
        original_size = len(df)
        df = df[df["file_path"].apply(os.path.exists)]
        if len(df) < original_size:
            print(f"   âš ï¸  {original_size - len(df)} adet bulunamayan ses dosyasÄ± atlandÄ±.")
        
        if len(df) == 0:
            raise ValueError(f"âŒ Hata: HiÃ§ geÃ§erli ses dosyasÄ± bulunamadÄ±!")
        
        # BoÅŸ transkriptleri filtrele
        df = df[df['transcript'].notna() & (df['transcript'].str.strip() != '')]
        
        dataset = Dataset.from_pandas(df)
        dataset = dataset.cast_column("file_path", Audio(sampling_rate=config.ORNEKLEME_ORANI, decode=False))
        
        print(f"   ğŸ“ˆ Veri seti boyutu: {len(dataset)} kayÄ±t")
        return dataset

    def train_model(self, dataset):
        """Model eÄŸitimini baÅŸlatÄ±r."""
        print("ğŸš€ KiÅŸiselleÅŸtirilmiÅŸ model eÄŸitimi baÅŸlÄ±yor...")
        print(f"   Epoch sayÄ±sÄ±: {config.NUM_FINETUNE_EPOCHS}")
        print(f"   Batch size: {config.FINETUNE_BATCH_SIZE}")
        print(f"   Learning rate: {config.FINETUNE_LEARNING_RATE}")
        print(f"   Gradient accumulation: {config.GRADIENT_ACCUMULATION_STEPS}")

        # Veri Ã¶n iÅŸleme
        num_proc = min(4, os.cpu_count() or 1)
        print(f"\nâš™ï¸  Veri Ã¶n iÅŸleme {num_proc} CPU Ã§ekirdeÄŸi ile paralelleÅŸtiriliyor...")
        
        try:
            processed_dataset = dataset.map(
                _standalone_preprocess_function,
                fn_kwargs={"processor": self.processor},
                remove_columns=dataset.column_names,
                batched=True,
                batch_size=config.FINETUNE_BATCH_SIZE,
                num_proc=num_proc
            )
            
            # BoÅŸ Ã¶rnekleri filtrele
            processed_dataset = processed_dataset.filter(
                lambda x: len(x.get("input_values", [])) > 0 and len(x.get("labels", [])) > 0
            )
            
            if len(processed_dataset) == 0:
                raise ValueError("âŒ Hata: Ã–n iÅŸleme sonrasÄ± hiÃ§ geÃ§erli Ã¶rnek kalmadÄ±!")
            
            print(f"   âœ… Ã–n iÅŸleme tamamlandÄ±. {len(processed_dataset)} geÃ§erli Ã¶rnek.")
            
        except Exception as e:
            print(f"âŒ Veri Ã¶n iÅŸleme hatasÄ±: {e}")
            import traceback
            traceback.print_exc()
            return

        # Data collator ve dataloader
        data_collator = DataCollatorCTCWithPadding(processor=self.processor)

        dataloader = DataLoader(
            processed_dataset,
            batch_size=config.FINETUNE_BATCH_SIZE,
            collate_fn=data_collator,
            shuffle=True
        )

        # Optimizer
        optimizer = AdamW(
            self.model.parameters(), 
            lr=config.FINETUNE_LEARNING_RATE,
            weight_decay=5e-3
        )
        
        # Accelerator (GPU desteÄŸi ve gradient accumulation iÃ§in)
        accelerator = Accelerator(
            mixed_precision="fp16" if torch.cuda.is_available() else "no",
            gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS
        )
        
        self.model, optimizer, dataloader = accelerator.prepare(
            self.model, optimizer, dataloader
        )

        # EÄŸitim dÃ¶ngÃ¼sÃ¼
        num_epochs = config.NUM_FINETUNE_EPOCHS
        num_training_steps = num_epochs * len(dataloader)
        progress_bar = tqdm(range(num_training_steps), desc="EÄŸitim")

        self.model.train()
        total_loss = 0.0
        
        try:
            for epoch in range(num_epochs):
                epoch_loss = 0.0
                num_batches = 0
                
                for step, batch in enumerate(dataloader):
                    with accelerator.accumulate(self.model):
                        outputs = self.model(**batch)
                        loss = outputs.loss
                        
                        accelerator.backward(loss)
                        optimizer.step()
                        optimizer.zero_grad()
                    
                    epoch_loss += loss.item()
                    num_batches += 1
                    total_loss += loss.item()
                    
                    progress_bar.update(1)
                    avg_loss = epoch_loss / num_batches
                    progress_bar.set_description(
                        f"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}"
                    )
                
                print(f"\n   Epoch {epoch+1}/{num_epochs} tamamlandÄ±. Ortalama Loss: {epoch_loss/num_batches:.4f}")

            print("\nâœ… Model ince ayarÄ± tamamlandÄ±!")
            
            # Model kaydetme
            unwrapped_model = accelerator.unwrap_model(self.model)
            self.output_dir.mkdir(parents=True, exist_ok=True)
            save_model_and_processor(unwrapped_model, self.processor, str(self.output_dir))

            print(f"ğŸ’¾ KiÅŸiselleÅŸtirilmiÅŸ model kaydedildi: {self.output_dir}")
            print(f"   Toplam eÄŸitim adÄ±mÄ±: {num_training_steps}")
            print(f"   Ortalama loss: {total_loss / num_training_steps:.4f}")
            
        except Exception as e:
            print(f"\nâŒ EÄŸitim sÄ±rasÄ±nda hata oluÅŸtu: {e}")
            import traceback
            traceback.print_exc()
            raise

def main():
    parser = argparse.ArgumentParser(description="KullanÄ±cÄ±ya Ã¶zel ASR modelini eÄŸitir.")
    parser.add_argument("user_id", type=str, help="Verisi kullanÄ±lacak ve modeli kiÅŸiselleÅŸtirilecek kullanÄ±cÄ±nÄ±n kimliÄŸi.")
    parser.add_argument("--base_model", type=str, help="Ä°nce ayar iÃ§in kullanÄ±lacak temel modelin yolu. VarsayÄ±lan: config.py'deki model.", default=None)
    
    args = parser.parse_args()
    
    trainer = PersonalizedTrainer(user_id=args.user_id, base_model_path=args.base_model)
    trainer.run()

if __name__ == "__main__":
    if torch.cuda.is_available():
        import multiprocess as mp
        mp.set_start_method("spawn", force=True)

    main()